\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

\usepackage{subcaption}
\usepackage[draft,inline,nomargin]{fixme} % Enable fixme-notes.
\usepackage{units}

\usepackage{array}
\usepackage{multicol}
\usepackage{multirow}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
\newcommand*\widebar[1]{%
  \begingroup
  \def\mathaccent##1##2{%
    \rel@kern{0.8}%
    \overline{\rel@kern{-0.8}\macc@nucleus\rel@kern{0.2}}%
    \rel@kern{-0.2}%
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
  \macc@nested@a\relax111{#1}%
  \endgroup
}
\makeatother

\graphicspath{{pictures/}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{RGB-Depth-Thermal Human Body Segmentation}

\author{Chris Bahnsen, Andreas MÃ¸gelmose, Thomas B. Moeslund\\
Aalborg University\\
Sofiendalsvej 11, 9200 Aalborg SV, Denmark\\
{\tt\small \{cb, am, tbm\}@create.aau.dk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we address the problem of human body segmentation from multi-modal visual cues. The generation of new multi-modal descriptors and fusion strategies aims to enrich the data representation in order to improve segmentation results. We therefore propose a multi-modal segmentation approach which, combining RGB, depth and thermal cues, automatically detects those regions of interest belonging to a human subject. To do so, several state-of-the-art descriptors have been tested and adapted to extract information from the different modalities. The different feature spaces are modeled via Gaussian Mixture Models for both subject and non-subject categories, providing a confidence score for each grid of region of interest. The feature descriptions have been tested independently to evaluate their performance for subject segmentation. In addition, three voting-based heuristic approaches have been proposed, along with a learning-based fusion approach that applies some state-of-the-art supervised algorithms for classification in a stacked learning fashion. Such proposal can act as a baseline for further developments. Furthermore, we present a novel registered multi-modal RGB-Depth-Thermal dataset of continuous image sequences, which has been manually annotated at pixel-level in all three modalities at those regions containing human subjects. Results show variable performance for the different modalities when segmenting people in multi-modal data, and improved segmentation accuracy of the multi-modal GMM- stacked learning method.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
Segmentation of people in images is a challenging computer vision problem. Main difficulties arise from the articulated nature of the human body, changes in appearance, lighting conditions, partial occlusions and the presence of background artifacts. Despite extensive research done so far, some constraints have still to be taken into account. One often has to make assumptions about the scenario where the segmentation task is to be applied, such as static versus moving camera, indoor versus outdoor, among others. Ideally, it should be tackled in an automatic fashion rather than relying on user intervention, which makes such task even more demanding. 

State-of-the-art methods that tackle the human body segmentation task mostly use color images recorded by RGB cameras as the main cue for further analysis, although they present several widely known intrinsic problems such as intensity similarities between background and foreground. More recently, the release of RGB-Depth devices such as Microsoft\textsuperscript\textregistered Kinect\textsuperscript\texttrademark has allowed the community to use RGB images along with per-pixel depth information. Furthermore, thermal imagery is becoming a complementary and affordable visual modality. Indeed, having different modalities and descriptions allow us to fuse them to have a more informative and richer representation of the scene. 

In this paper we present a novel tri-modal continuous image dataset of RGB-Depth-Thermal data that contains up to three different people appearing concurrently in three different indoor scenarios, performing diverse actions that involve interaction with objects. The dataset is presented along with an algorithm that performs the calibration and registration among modalities. In addition, we propose a baseline methodology to automatically segment human subjects appearing in the tri-modal video sequences. We start reducing the search space by learning a model of the background to subsequently perform background subtraction, thus segmenting subject candidate regions in all the available and registered modalities. Then, such regions are described using simple but reliable uni-modal feature descriptors. These descriptors are used to learn probabilistic models so as to predict the candidate region actually belonging to people. Predictions got from the different models are fused using both heuristic and learning-based approaches. We compare results from applying segmentation to the different modalities separately to results obtained by fusing features from all modalities. Experimental results demonstrate the effectiveness of the proposed algorithms to perform registration among modalities and segment human subjects. To the best of our knowledge, this is the first dataset and work that combines color, depth and thermal modalities to perform the people segmentation task in videos, aiming to bring further benefits towards developing more robust solutions.

The remainder of this paper is organized as follows: Section~\ref{sec:relatedwork} reviews the different approaches for human body segmentation that appear in the recent literature. Section~\ref{sec:setup} explains the acquisition and multi-modal calibration process, whereas Section~\ref{sec:registration} describes the registration algorithm between modalities. Section~\ref{sec:trimodalhumanbodysegmentation} presents the proposed baseline methodology, which will be experimentally evaluated in Section~\ref{sec:evaluation} along with the registration algorithm. Finally, Section~\ref{sec:conclusions} concludes the paper.

\section{Related work}
\label{sec:relatedwork}
 
 When dealing with indoor scenarios recorded by a stationary camera, the pixel-based background subtraction approach can be applied successfully. One can model the background distribution of the scene and detect moving objects by comparing each pixel to the model, which are considered as foreground. The result is a silhouette of the moving object, which can be further used for other tasks. Pixel intensity is the most commonly used feature in background modeling, though there are many approaches that use other type of information such as edge, motion, stereo or texture features. The parametric model that Stauffer and Grimson proposed in \cite{stauffer1999adaptive}, which models the background using a mixture of gaussians (MoG), has been widely used and many variations have been suggested based on it. In \cite{bouwmans2011recent}, more advanced statistical background modeling techniques are deeply reviewed. 
 
Nonetheless, after obtaining the moving object contours one still needs a way to assess whether they belong to a human or not. Human detection methods are strongly related to the task of human body segmentation since they allow to discriminate better between other objects. They usually produce a bounding box indicating where the person is, which in turn may be also useful as a prior for pixel-based or bottom-up approaches to refine the final human body silhouette. In the category of holistic body detectors, one of the most successful representations is the Histogram of Oriented Gradients (HOG) \cite{dalal2005histograms}, still being the basis of many current detectors. Used along with a discriminative classifier --e.g. Support Vector Machines (SVM) \cite{hearst1998support} --it is able to accurately predict the presence of human subjects. Example-based methods \cite{andriluka2010monocular} have been also proposed to address human detection, utilizing templates to compare the incoming image and locate the person, limiting the pose variability though.  

Regarding descriptors, other possible representations apart from the already commented HOG are those that try to fit the human body into silhouettes \cite{mittal2003human}, those that model color or texture such as Haar-like wavelets \cite{viola2005detecting}, optical flow quantized in Histrograms of Optical Flow (HOOF) \cite{dalal2006human} and, more recently, descriptors including logical relations, e.g. Grouplets \cite{yao2010grouplet}, which enables to recognize human-object interactions.

Instead of whole body detection, some approaches have been built under the assumption that the human body consists of an ensemble of body parts \cite{ramanan2006learning, pirsiavash2012steerable}. Some of them are based on pictorial structures \cite{andriluka2009pictorial, yang2011articulated}. In particular, \cite{yang2011articulated, yang2012articulated} from Yang and Ramanan along with \cite{felzenszwalb2010object} from Felzenszwalb have outperformed other existing methods using a Deformable Part-based Model (DPM) that consists on a root HOG-like filter and different part-filters that define a score map of an object hyphotesis, using latent SVM as a classifier. Another well-known part-based detector is Poselets \cite{bourdev2009poselets, wang2011learning}, which trains different homonymous parts to fire at a given part of the object at a given pose and viewpoint. More recently, Motionlets \cite{wang2013motionlets} have been proposed for human motion recognition. Grammar models \cite{girshick2011object} and AND-OR graphs \cite{zhu2008max} have been also used in this context.

By the same token, other approaches model objects as an ensemble of local features. In this category there are included methods such as Implicit Shape Models (ISM) \cite{leibe2004combined}, consisting of visual words combined with location information. In addition, they are used in works that estimate the class-specific segmentation based on the detection result after a training stage \cite{leibe2008robust}.  

Conversely, generative classifiers directly deal with the person segmentation problem. They function in a bottom-up manner, learning a model from a initial prior in the form of bounding boxes or seeds, and using it to yield an estimate for the background and target distributions, normally applying Expectation Maximization (EM) \cite{shi2000normalized, carson2002blobworld}. One of the most popular is GrabCut \cite{rother2004grabcut, gulshan2011humanising}, an interactive segmentation method based on graph cuts \cite{boykov2001interactive} and Conditional Random Fields (CRF) that, using a bounding box as an initialization region, combines pixel appearance information with neighborhood relations to refine silhouettes up to a very accurate level. 
%When the scenario is static camera sequences are used, pixel-based approaches play an important role 
 % It is robust to illumination and local contrast changes and scale invariant. 
 
Having considered the properties of each one of the aforementioned segmentation categories, it is reasonable that several approaches have been proposed towards their combination, that is, top-down and bottom-up segmentation \cite{lin2007interactive, mori2004recovering, ladicky2010and, levin2006learning, fidler2013bottom}. Just to name a few, ObjCut \cite{kumar2005obj} combines pictorial structures and Markov Random Fields (MRF) to obtain the final segmentation. PoseCut \cite{bray2006posecut} is also based on MRF and graph cuts but it has the added ability to deal with 3D pose estimation from multiple views.
 
 Most of the aforementioned contributions use RGB as the principal cue to extract the corresponding descriptors. The recent release of affordable RGB-Depth devices such as Microsoft\textsuperscript\textregistered Kinect\textsuperscript\texttrademark has encouraged the community to start using depth maps as a new source of information. \cite{shotton2011depth} was one of the first contributions, which used depth images to extract the human body pose, an approach that is also the core of the Kinect\textsuperscript \texttrademark human recognition framework. 
 
 A number of standard computer vision descriptors already discussed for color cues have also been applied to depth maps. For instance, a combination of Graph cuts method and Random Forest has been further applied to part-based human segmentation  \cite{hernandez2012graph}. The authors of \cite{holt2011putting} proposed the usage of poselets as a representation that combines part-based and example-based estimation aspects for human pose estimation. Generative models have also been considered, such as in \cite{charles2011learning}, where they are used to learn limb shape models from depth, silhouette and 3D pose data. Active Shape Models (ASM), Gabor filters \cite{pugeault2011spelling}, template matching, geodesic distances \cite{schwarz2011estimating} and linear programming \cite{windheuser2011geometrically} have been also employed in this context.
 
Notwithstanding the former, the emergence of the depth modality has lead to the design of novel descriptors. Just to name a few, the authors of \cite{plagemann2010real} proposed key point detector based on saliency of depth maps for identifying body parts. Fast Point Feature Histrogram (FPFH), based on the extraction of normal vectors taking profit of 3D point cloud representation, has also been proposed \cite{rusu2009fast, rusu20113d, mogelmosetri}. In \cite{xia2011human}, the authors applied 2D Chamfer match over silhouettes for human detection and segmentation based on contouring depth images. A recent contribution is the Histogram of Oriented 4D Normals (HON4D) \cite{oreifej2013hon4d}, which proposes a histogram capturing the distribution of the surface normal orientations in the 4D space of depth, time and spatial coordinates.
 
Given the increasing popularity of such kind of imagery, it is not surprising that a number of algorithms that combine both depth and RGB cues have appeared to benefit from the multi-modal data representation \cite{stefanczyk2012multimodal,  clapes2012user, sheasby2012simultaneous, hernandez2012bovdw, teichman2013learning, scharwachter2013efficient, sheasby2013robust, alahari2013pose}. A recent example is PoseField \cite{vineet2013posefield}, a filter-based mean-field inference method that jointly estimates human segmentation pose, per-pixel body parts, and depth given stereo pairs of images. Indeed, disparity computation from stereo images is another wide used approach to obtain depth maps without range and outdoor limitations. Even background subtraction approaches can take profit from such fusion, since it is possible to reduce those misdetections that cannot be tackled by each modality individually \cite{gordon1999background, camplani2014background}. 
 
 In contrast to color or depth cues, thermal infrared imagery has not been used that widely for segmentation purposes, although it is experiencing a growing interest by the research community. Several specific descriptors have been proposed so far. For instance, HOG and SVM is used in \cite{suard2006pedestrian}, whereas the authors of \cite{zhang2007pedestrian} extended such combination with edgelets and Adaboost. Other examples include joint shape and appearance cues \cite{dai2007pedestrian}, probabilistic models \cite{bertozzi2007pedestrian}, Shape Context Descriptor (SCD) with Adaboost \cite{wang2010improved}, or descriptors invariant to scale, brightness and contrast \cite{olmeda2012contrast}. Background subtraction has been also adapted to deal with this kind of imagery in \cite{davis2004robust}, which presented a statistical contour-based technique that eliminates typical halo artifacts produced by infrared sensors by combine foreground and background gradient information into a contour saliency map so as to find the strongest salient contours. An example of human segmentation is found in \cite{fernandez2011real}, which applies thresholding and shape analysis methods to perform such task.

As one can observe, most of the cited contributions are focused on pedestrian detection applications. Indeed, thermal imaging has attracted the most attention for occupancy analysis \cite{gade2013long} and pedestrian detection applications, due to the cameras' ability to see without visible illumination and the fact that people cannot be identified in thermal images, thus eliminating privacy issues. An extensive survey of thermal cameras and more applications can be found in \cite{gade2014thermal}, including technological aspects and the nature of thermal radiation. 

Similarly to the RGB-D combination, thermal imaging has also been fused with color cues to enrich the data representation. Such combination has been applied to pedestrian tracking \cite{leykin2006robust, leykin2007thermal}, background subtraction \cite{davis2007background}. Human body segmentation is tackled in \cite{zhao2012human}, which also explains the calibration procedure and the geometric registration between visual blobs corresponding to human subjects observed at both cameras. 

Eventually, several approaches have considered the fusion of RGB, depth and thermal features (RGB-D-T) in order to improve detection and classification abilities. The latest contributions include people following \cite{susperregi2013rgb}, human motion tracking \cite{chun2013applications}, person re-identification \cite{mogelmosetri} and, more recently, face recognition \cite{nikisinsrgb}. However, little attention has been paid to human segmentation applications combining such cues.

Despite the extensive review of methods and related work, our contribution focuses on fast and reliable descriptors for multi-modal segmentation. Such task is often used as a first step for further sophisticated pose and behavior analysis approaches. To advance research in this area, it is necessary to have the right means to compare existent methods so as to allow improvements to be measured. There exist several static and continuous image-based human-labeled datasets that can be used for that purpose \cite{moeslund2011visual}, which try to provide realistic settings and environmental conditions. The best known of these is the Berkeley Segmentation dataset and Benchmark \cite{martin2001database}, which consists of 12,000 segmentations of 1,000 Corel dataset color images, containing people or different objects. It also includes figure-ground labelings for a subset of the images. Authors of \cite{alpert2007} made also available a database containing 200 gray level images along with ground truth segmentations, which was specially designed to avoid potential ambiguities by only incorporating images that clearly depict one or two objects in the foreground that differ from its surroundings by either texture, intensity, or other low level cues, but it does not represent uncontrolled scenarios. The well known PASCAL Visual Object Classes Challenge \cite{everingham2012pascal} tended to include a subset of the color images annotated in a pixel-wise fashion for the segmentation competition. Although not considered to be benchmarks, Kinect-based datasets are also available, since this device is being widely used in human pose related works. In \cite{gulshan2011humanising} a novel dataset was presented, which contains 3,386 images of segmented humans and ground truth automatically created by Kinect, and consisting of different human subjects across 4 different locations. Unfortunately, depth map images are not included in the public dataset. 

Nevertheless, little attention has been given to multi-modal video datasets. We underline the collective datasets of Project ETISEO \cite{nghiem2007etiseo}, owing to the fact that for some of the scenes the authors include, apart from color images, an additional imaging modality such as infrared footage. It consists of indoor and outdoor scenes of public places such as an airport apron or a subway station, and also includes a frame-based annotated ground truth. Depth maps computed from stereo pairs of images are used in Inria 3D Movie dataset \cite{alahari2013pose}, which contains sequences from 3D movies. Such sequences show various people performing a broad variety of activities from a range of orientations and with different levels of occlusions. A comparison of existing multi-modal datasets focused on human body related approaches is provided in Table \ref{tab:datasets}. As one can observe, there is a lack of datasets combining RGB, depth and thermal modalities focused on the human body segmentation task, which we propose in this paper. Such dataset is described in the next section.  

\begin{table*}[htpb]
\centering
\begin{tabular}{p{4cm}p{1.5cm}p{0.7cm}p{3cm}p{1.2cm}p{4.1cm}}
\hline
dataset & Data \par Format\strut & Video \par Seq.\strut & Annotation & Scenario & Purpose \\ \hline
ETISEO Project \cite{nghiem2007etiseo} & RGB-T & Yes & Bounding Box & Indoor/ \par Outdoor\strut & Video Surveillance \\
IRIS Thermal/Visible \par Face Database \cite{iris2007dataset}\strut	&	RGB-T	&	No	&	-	&	Indoor	&	Face Detection \\
OSU Color-Thermal \par Database \cite{davis2007background}\strut	&	RGB-T	&	Yes	&	Bounding box	&	Outdoor	&	Object Detection \\
RGB-D People dataset \cite{spinello2011people}	&	RGB-D	&	Yes	&	Bounding Box	&	Indoor	&	Human Detection \\
H2View dataset \cite{sheasby2012simultaneous}	&	RGB-D  \par(stereo)\strut 	&	Yes	&	Segmentation masks, \par Ground-truth depth,\strut \par Human pose\strut	&	Indoor	&	3D Pose Estimation \\
LIRIS Human activities \par dataset \cite{wolf2012liris}\strut	&	RGB-D	&	Yes	& Bounding box, \par Activity class\strut	&	Indoor	&	Human Activity Recognition \\
RGB-D Person \par Re-identification dataset \cite{barbosa2012re}\strut	&	RGB-D	&	Yes	&	Foreground masks, \par Skeleton,\strut \par3D mesh\strut 	&	Indoor	&	Person Re-identification \\
VAP RGB-D Face \par dataset \cite{hg2012rgb}\strut	&	RGB-D	&	No	&	Pose class	&	Indoor	&	Face Detection, \par Pose Estimation\strut \\
Biwi Kinect Head Pose \par Database \cite{fanelli2013random}\strut	&	RGB-D	&	Yes	&	Head 3D position, \par Head rotation\strut	&	Indoor	&	Head Pose Estimation \\
Cornell Activity \par datasets \cite{koppula2013learning}\strut	&	RGB-D	&	Yes	&	Bounding box, \par Activity class,\strut \par Skeleton\strut	&	Indoor	&	Human Activity Recognition \\
Eurecom Kinect Face \par dataset \cite{huynh2013efficient}\strut	&	RGB-D	&	No	&	6 facial landmarks, \par Person information\strut	&	Indoor	&	Face Recognition \\
Inria 3D Movie dataset \cite{alahari2013pose}	&	RGB-D \par(stereo)\strut 	&	Yes	&	Bounding box, \par Human pose,\strut \par Segmentation masks\strut	&	Indoor/ \par Outdoor\strut	&	Human Detection, \par Human Segmentation,\strut \par Pose Estimation\strut \\
RGB-D-T Facial \par Database \cite{nikisinsrgb}\strut &	RGB-D-T	&	No	&	Bounding box	&	Indoor	&	Face Recognition \\
\hline
\end{tabular}
\caption{Comparison of multi-modal datasets aimed for human body related approaches in order of release.}
\label{tab:datasets}
\end{table*}
 
\section{The tri-modal dataset}
\label{sec:dataset}

The proposed method is evaluated on a dataset with a total of 11537 frames, of which 5724 frames are annotated. Sample imagery of the scenes is pictured in Figure \ref{fig:samplescenes} whereas Table \ref{tab:scenes} shows the number of annotated frames and the depth range. Activity in scene 1 and 3 is using the full depth range of the Kinect sensor whereas activity in scene 2 is constrained to a depth range of $\pm 0.250$ m. Scene 1 and 2 are situated in a closed meeting room with little natural light to disturb the depth sensing, whereas scene 3 is situated in a area with wide windows and a substantial amount of sunlight.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.48\textwidth]{Selection/1.png}
		\includegraphics[width=0.48\textwidth]{Selection/2.png}
		\includegraphics[width=0.48\textwidth]{Selection/3.png}
	\caption{Two views of each of the three scenes shown in the RGB, thermal, and depth modalities, respectively.}
	\label{fig:samplescenes}
\end{figure}

\begin{table}[htpb]
\centering
\begin{tabular}{llll}
\hline
Scene & Frames 	& Annotated frames 	& Depth range \\ \hline
1 		& 4693		& 1767	 						& 1-4 m \\
2 		& 2216		& 2016	 						& 1.4-1.9 m \\
3 		& 4628		& 1941	 						& 1-4 m \\ 
\hline
\end{tabular}
\caption{Annotated number of frames and spatial constraints of the scenes.}
\label{tab:scenes}
\end{table}


\subsection{Acquisition}
\label{sec:setup}
The tri-modal data stream is recorded using a Microsoft Kinect for XBOX 360 capturing the RGB and depth image streams, and an AXIS Q1922 thermal camera. The resolution of the imagery is fixed at 640x480 pixels. As seen in Figure \ref{fig:camerasetup}, the cameras are vertically aligned in order to reduce perspective distortion. 

\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.25\textwidth]{pictures/camerasetup.jpg}
	\caption{Camera configuration. The RGB and thermal sensor are vertically aligned.}
	\label{fig:camerasetup}
\end{figure}

The image streams are captured using custom recording software that invokes the Kinect for Windows and AXIS Media Control SDKs. The integration of the two SDKs enables the cameras to be calibrated against the same system clock, which enables the post-capture temporal alignment of the image streams. Both cameras are able to record at 30 FPS. However, the dataset is recorded at 15 FPS due to performance constraints of the recording software. 

\subsection{Multi-modal calibration}
The calibration of the thermal and RGB cameras have been accomplished using a thermal-visible calibration device inspired by \cite{vidas2012mask}. The calibration device consists of two parts; an A3-sized 10 mm polystyrene foam board is used as a backdrop and an equally sized board with cut-out squares is used as the checkerboard. Before using the calibration device, the backdrop is heated and the checkerboard plate is kept at room temperature, thus keeping a suitable thermal contrast when joined, as is seen from Figure \ref{fig:calibrationDevice}. %The depth sensor of the Kinect is factory calibrated and a subsequent calibration is thus not needed.
Using the Camera Calibration Toolbox of \cite{bouguet2004camera}, we are able to extract corresponding points in the thermal and RGB modalities. The sets of corresponding points are used to undistort both image streams and for the subsequent registration of the modalities. 

\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{RGB00064.png}%
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{T00064.jpg}%
	\caption{}%
\end{subfigure}
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{plane1.pdf}%
	\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{plane3.pdf}%
	\caption{}%
\end{subfigure}
\caption{The calibration device as seen by the (a) RGB and (b) thermal camera. The corresponding points in world coordinates is seen in (c) and the plane, which induces an homography, is overlayed in (d). Noise in the depth information accounts for the outliers in (c) and (d).}
\label{fig:calibrationDevice}
\end{figure}


\subsection{Registration}
\label{sec:registration}
The depth sensor of the Kinect is factory registered to the RGB camera and a point-to-point correspondence is obtained from the SDK. The registration is static and might thus be saved in two look-up-tables for $\text{RGB} \Leftrightarrow \text{depth}$. %The LUT shows to be inaccurate at positions where the Kinect cannot estimate the depth. This inaccuracy can be solved, however, by smoothing the LUT by a linear polynomial. 

The registration from $\text{RGB} \Rightarrow \text{thermal}$, $\mathbf{x} \Rightarrow \mathbf{x}'$, is handled using a weighted set of multiple homographies based on the approximate distance in space to the view that the homography represents. By using multiple homographies, we are allowed to compensate for parallax at different depths. However, the spatial dependency of the registration implies that no fixed, global registration or look-up-table is possible, thus inducing a unique mapping for each pixel at different depths.

Homographies relating RGB and thermal modalities are generated from minimum 50 views of the calibration device scattered throughout each scene. One view of the calibration device induces 96 sets of corresponding points in the RGB and thermal modality (Figure \ref{fig:calibrationDevice}c) from which a homography is computed using a RANSAC-based method. The acquired homography and the registration it establishes is only accurate for points on the plane that is spanned by the particular view of the calibration device. To register an arbitrary point of the scene, $\mathbf{x} \Rightarrow \mathbf{x}'$, the eight closest homographies are weighted according to this scheme:

\begin{enumerate}
	\item For all $J$ views of the calibration device, calculate the 3D centre of the $K$ extracted points in the image plane:
\begin{equation}
\widebar{\mathbf{X}}_{j} = \frac{\sum_{k=1}^K \mathbf{X}_{k_j}}{K} = \frac{\sum_{k=1}^K \mathbf{P}^+ \mathbf{x}_{k_j}}{K}.
\end{equation}
The depth coordinate of $\mathbf{X}$ is estimated from the registered point in the depth image. $\mathbf{P}^+$ is the pseudoinverse of the projection matrix.
\item Find the distance from the reprojected point $\mathbf{X}$ to the homography centres:
\begin{equation}
\omega(j) = \lvert \mathbf{X} - \widebar{\mathbf{X}}_{j} \rvert.
\end{equation}
\item Centre a 3D coordinate system around the reprojected point $\mathbf{X}$ and find $\min \omega(j)$ for each octant of the coordinate system. Set $\omega(j) = 0$ for all other weights. Normalize the weights:
\begin{equation}
\omega^*(j) = \frac{\omega(j)}{\sum_{j=1}^J \omega(j)}.
\end{equation}
\item Perform the registration $\mathbf{x} \Rightarrow \mathbf{x}'$ by using a weighted sum of the homographies:
\begin{equation}
\mathbf{x}' = \sum_{j=1}^J \omega^*(j) \ \mathbf{H}_j \mathbf{x},
\end{equation}
where $\mathbf{H}_j$ is the homography induced by the j\textsuperscript{th} view of the calibration device.
\end{enumerate}

%The spatial dependency of the registration algorithm implies that no fixed registration or look-up-table is possible. Thus, in order to register an image, one must know the spatial properties of each pixel, including depth information. 
For registering thermal points, the absence of depth information means that points are reprojected at a fixed distance, inducing parallax for points at different depths. Thus, the registration framework may be written:
\begin{align}
\text{depth} \Leftrightarrow \text{RGB} \Rightarrow \text{thermal}
\label{eq:mappingDiagram}
\end{align}

The accuracy of the registration of $\text{RGB} \Rightarrow \text{thermal}$ is mainly dependent on: 
\begin{enumerate}
	\item The distance in space to the nearest homography. %In theory, the error is proportional to the distance to the plane spanned by the homography; in practice, however, the Euclidean distance to the centre is a better estimate. 
	\item The synchronization of thermal and RGB cameras. At 15 FPS, the maximal theoretical temporal misalignment between frames is thus 34 ms. 
	\item The accuracy of the depth estimate.
\end{enumerate}
An example of the registration is seen from Figure \ref{fig:registeredImagery}. 

\begin{figure}[htpb]
\centering
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{RGBregistered.png}%
	\caption{}%
	\label{}%
\end{subfigure}
\begin{subfigure}[b]{0.48\columnwidth}
	\includegraphics[width=\columnwidth]{Thermalregistered.png}%
	\caption{}%
	\label{}%
\end{subfigure}
\caption{Example of $\text{RGB (a)} \Rightarrow \text{thermal (b)}$ registration.}
\label{fig:registeredImagery}
\end{figure}

\subsection{Annotation}
%The frames are annotated using a pixel annotator tool ***ANDREAS TEXT HERE****
%
 %For each scene, human objects are annotated in the RGB modality. Each person is given a unique ID which he maintains throughout the scene. To obtain the corresponding masks in the thermal and depth modalities, the RGB masks are mapped using the registration algorithm of Section \ref{sec:registration}.
The acquired videos were manually annotated frame-by-frame in a custom annotation program called Pixel Annotator. The dataset contains a large number of frames divided over a number of different sequences. All sequences have three modalities: RGB, depth, and thermal. The focus of the annotation has been on the people in the scene and a mask-based annotation philosophy has been employed. That means that each person is covered by a mask and each mask (person) has a unique ID, which is consistent over frames. In this way the dataset can be used not only for background subtraction, but also for tracking and re-identification purposes. Since the main purpose of the dataset is background subtraction, a pixel-level annotation scheme was necessary - bounding boxes would not be sufficient.
 
As seen from Figure \ref{fig:pixelannotator}, Pixel Annotator provides a view of each modality with the current mask overlaid, as well as a raw view of the mask. It implements the registration algorithm described above so that the annotator can judge whether the mask fits in all modalities. Because the modalities are registered to each other, there is not specific masks for each modality, but a single mask for all.

\begin{figure}%
\includegraphics[width=0.48\textwidth]{pixelannotator2.png}%
\caption{Pixel Annotator showing the RGB masks and the corresponding, registered masks in the other views.}%
\label{fig:pixelannotator}%
\end{figure}

Each annotation can be initialized with an automatic segmentation using the GrabCut algorithm \cite{rother2004grabcut} to get quickly off the ground. Then Pixel Annotator provides pixel-wise editing functions to further refine the mask. Each annotation is associated with a numerical ID, and can have an arbitrary number of property fields associated with it. They can be boolean or contain strings, so advanced annotation can take place, all the way from simple occluded/not occluded fields to fields describing the current activity. Pixel Annotator is written in C++ on the Qt framework and is fully cross-platform compatible.

The dataset and the registration algorithm is freely available at \cite{vapgroup}.


\section{Tri-modal human body segmentation}
\label{sec:trimodalhumanbodysegmentation}

% TODO: correct the baseline's figure, removing the objects' GMM

We propose a baseline methodology to segment human subjects automatically in multi-modal video sequences. The first step of our method focuses on reducing the search space by estimating the scene background to extract the foreground regions of interest in each one of the modalities. Note that such regions may belong to human subjects or non-human subjects, so in order to perform an accurate classification we describe them using  modality-specific state-of-the-art feature descriptors. The obtained features are then used to learn probabilistic models in order to predict which foreground regions actually belong to human subjects. Predictions got from the different models are fused using both heuristic and learning-based approaches. Figure \ref{fig:baseline} depicts the different stages of the method.
%The method consists in several steps. First, ... blablabla. Then, blablabal ... In Fig.~\ref{fig:baseline}, the different steps of the baseline up to this point are illustrated.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{pictures/diagram.png}
	\caption{The main steps of the proposed baseline method, before reaching the fusion step.}
	\label{fig:baseline}
\end{figure}

%Let us write $\mathbf{F}_i = \{\mathbf{C}_i, \mathbf{D}_i, \mathbf{T}_i\}$ for a determined tri-modal frame, and $\mathbf{p}$ a pixel at an arbitrary location $(x,y)$ in an image.
%continue this intro

\subsection{Extraction of masks and regions of interest} 
\label{ssec:bsbb}

The first step of our baseline is to attempt to reduce the search space. For this task, we learn a model of the background and perform background subtraction.

\subsubsection{Background subtraction}
\label{sect:bs}
 A widely used approach for background modeling in this context is Mixture of Gaussians MOG  \cite{bouwmans2008background}, which assigns a Gaussian Mixture Model (GMM) per pixel with a fixed number of components. Sometimes background presents periodically moving parts such as noise or sudden and gradual illumination changes. Such problems are often tackled with adaptive algorithms that keep learning the pixel's intensity distribution after the learning stage with a decreased learning rate. However, this also causes that intruding objects that stand still for a period of time vanish, so in our case a non-adaptive approach is more convenient.

Although this background subtraction technique performs fairly well, it has to deal with the intrinsic problems of the different image modalities. For instance, color-based algorithms may fail due to shadows, similarities in color between foreground and background, highlighted regions, and sudden lighting changes. Thermal imagery may also have this kind of problems, plus the inconvenience of temperature changes in objects. A halo effect is also observed around warm items. Regarding to depth-based approaches, they may produce misdetections due to the presence of foreground objects with similar depth to the background. Depth data is quite noisy and many pixels in the image may have no depth due to multiple reflections, transparent objects, or scattering in certain surfaces such as human tissue and hair. Furthermore, a halo effect around humans or objects is usually perceived due to parallax issues. However, they are more robust to lighting artifacts and shadows. A comparison is shown in Fig. \ref{fig:bscomparison}, where the actual foreground objects are the human and the objects on the table. As one can observe, RGB fails at extracting the human legs due to the similarities in color with the chair at the back. The thermal cue segments the human body more accurately, but includes some undesired reflections and the jar and mugs with a surrounding halo. The pipe tube is also extracted as foreground due to temperature changes along time. Despite its drawbacks, depth-based background subtraction is the one that seems to give the most accurate result. 

Therefore, the binary foreground masks of our proposed baseline are computed applying background subtraction to the depth modality previously registered to the RGB one, thus allowing us to use the same masks for both modalities.  Let us consider the depth value of a pixel at frame $i$ as $z_i$. The background model $p(z_i|B)$ --where $B$ represents the background -- is estimated from a training set of depth images represented by $\mathcal{Z}$ using the $T$ first frames of a sequence such that $\mathcal{Z} = \{z_1, \ldots, z_T\}$. This way, the estimated model is denoted by $\hat{p}(z_i| \mathcal{Z}, B)$, modeled as a mixture of gaussians. In particular, we use the method presented in \cite{zivkovic2004improved}, which uses an on-line clustering algorithm that constantly adapts the number of components of the mixture for each pixel during the learning stage. 

 \begin{figure}[!h]
{\includegraphics[width=\linewidth]{bs.eps}}
\caption{Background subtraction for different visual
modalities of the same scene (RGB, depth and thermal respectively).
\label{fig:bscomparison}}
\end{figure}

\subsubsection{Regions of interest extraction}
\label{sssec:extreg}
Once the binary foreground masks are obtained, a 2D connected component analysis is performed using basic mathematical morphological operators. We also set a minimum value for each connected component area (except in left and rightmost sides of the image which may be caused by a new incoming item) to clean the noisy output mask. 

A region of interest should contain a separated person or object. However, different subjects or objects may overlap in space, resulting in a bigger component containing more than one item. For this reason, each component has to be analyzed to find each item separately to obtain the correct bounding boxes that surround them.

One of the advantages of the depth cue is that we can use the depth value in each pixel to know whether an item is farther or not than other. We can assume that a given connected component denotes just one item if there is no rapid change in the disparity distribution and it has a low standard deviation. For those components that do have a greater standard deviation, and assuming a bimodal distribution -two items in that connected component-, Otsu's method \cite{otsu1975threshold} can be used to split the blob by automatically finding a threshold. It calculates the optimal threshold separating the two classes such that their intra-class variance is minimal. 

%We will define $\pi$ as the function that applies this method to a set of bounding boxes.

For such purpose, we define $\mathbf{c}$ as a vector containing the depth range values that correspond to a given connected component, with mean $\mu_{c}$ and standard deviation $\sigma_{c}$, and $\sigma_\mathrm{otsu}$ as a parameter that defines the maximum $\sigma_{c}$ allowed to not apply Otsu. Note that erroneous white or black pixels do not have to be taken into account in $\mathbf{c}$ when finding the Otsu's threshold because they would change the disparity distribution, thus leading to incorrect divisions. Hence, if $\sigma_{c} > \sigma_\mathrm{otsu}$, Otsu is applied. However, the assumption of bimodal distribution may not hold, so to take into account the possibility of more than two overlapping items the process is applied recursively to the divided regions in order to extract all of them. 

Once the different items are found, the regions belonging to them are labeled using a different id per item. Besides, rectangular bounding boxes are generated encapsulating such items individually over time, whose function is to denote the regions of interest of a given foreground mask. 

%This way, we define the set of bounding boxes of the $i$-th frame generated from the depth-based masks as $B^\mathrm{depth}_i = \{b_{ij}\text{ }|\text{ }\forall j = \{1, \ldots, n\} \}$, being $b_{ij}$ the $j$-th bounding box and $n$ the number of bounding boxes generated in that frame, which is equal to the number of items.


\subsubsection{Correspondence to other modalities} 
\label{sssec:correspondence}
%in Section \ref{sect:bs} 
As stated previously in Section \ref{sect:bs}, depth and color cues use the same foreground masks, so we can take advantage of the same bounding boxes for both modalities. On another front, foreground masks for the thermal modality are computed using the provided registration algorithm with the depth/color foreground masks as input. For each frame, each item is registered individually to the thermal modality and then merged into one mask, thus preserving the same item id than depth/color foreground masks. This way, we achieve a one-to-one straightforward correspondence between items of all modalities, and the constraint of having the same number of items in all the modalities is fulfilled. Bounding boxes are also generated in the same way as done for the depth modality, which although not having the same coordinates denote the same regions of interest. Henceforth, we will simply use $R$ to refer to such regions. From now on, we will use $F = \{F^\mathrm{color}, F^\mathrm{depth}, F^\mathrm{thermal}\}$ to refer to the set of foreground masks, noting that $F^\mathrm{color} = F^\mathrm{depth}$.

\subsubsection{Tagging regions of interest}
\label{sssec:tagging}
The extracted regions of interest are further analyzed to decide whether they belong to objects or subjects. In order to train and test our models and determine final accuracy results, we need to have a ground truth labeling of the bounding boxes apart from the ground truth masks. 

This labeling is done in a semiautomatic manner. First, bounding boxes are extracted from regions of interest of ground-truth masks and compared to those extracted previously from the foreground masks $F$ and the overlap between them is computed. Defining $y_r$ as the label applied to the $r$ region of interest, the automatic labeling is therefore applied as follows: 

\begin{gather}
\text{$y_r$} = \left\{ 
  \begin{array}{c l l}
    0  & \text{(Object)} & \quad \text{if overlap $\leq$ 0.1 }\\
    -1 & \text{(Unknown)} & \quad \text{if 0.1 $<$ overlap $<$ 0.6}\\
    1 & \text{(Subject)} & \quad \text{if overlap $\geq$ 0.6 }
  \end{array} \right.
 \end{gather}

This way, regions with low overlap are considered to be objects, whereas those with high overlap are classified as subjects. A special case named \emph{unknown} has been added to denote those regions that are not clear to classify directly, such as regions with subjects holding objects, multiple subjects overlapping, and so on. 

However, such conditions may not always hold, since some regions which overlap value is lower than 0.1 compared to the ground-truth masks could actually be part of human beings. It is for that reason that the applied labels are revised manually to check for possible mislabelings.

\subsection{Grid partitioning}
\label{ssec:gridpartitioning}

Given the precision got in the registration, particularly because of the depth-to-thermal transformation, we are not able to make a pixel-to-pixel correspondence among all the modalities. Instead, the association is made among greater information units: grid cells. 

Each region of interest $r \in R$ associated to either a segmented subject or object is partitioned in a grid of $n \times m$ cells. Let $G_r$ denote the grid, which in turn is a set of cells, corresponding to the region of interest $r$. Hence, we write $G_{rij}$ to refer to the position $(i,j)$ in the $r$-th region, such that $i \in \{ 1,...,n \}$ and $j \in \{ 1,...,m \}$. 
%Regarding to the whole set of $(i,j)$-cells $\{G_{rij}\}_{\forall r \in R}$, it is denoted by $G_{ij}$.

Besides, a grid cell $G_{rij}$ consists of a set of images $\{\mathbf{G}_{rij}^{(c)} \,|\, \forall{c} \in \mathcal{C}\}$, corresponding to the set of imagery cues $\mathcal{C} = \{\mathrm{``color"}, \mathrm{``depth"}, \mathrm{``thermal"}\}$. Accordingly, $G_{ij}^{(c)} = \{\mathbf{G}_{rij}^{(c)} \,|\, \forall r \in R\}$, the set of $(i,j)$-cells in the modality $c$, is indicated by $G_{ij}^{(c)}$.

The grid cell is the unit of information processed in the feature extraction and classification procedures. Next section provides the details about the feature extraction computed from the different visual cues at this cell level.

\subsection{Feature extraction}
\label{ssec:feature extraction}

%\begin{align}
%\begin{split}
%f \colon & \mathbbm{R}^{n \times m} \to \mathbbm{R}^\delta \\
% \mathbf{G} \xmapsto{\phantom{\mathbbm{R}^{n \times m}}} \mathbf{d}
%\end{split}
%\end{align}

Each modality involves its own specific feature extraction/description processes. In fact, a feature extraction process can be seen as a mathematical function $f$ such that $f \colon \mathbbm{R}^{n \times m} \to \mathbbm{R}^\delta$. Accordingly, $\mathbf{G} \xmapsto{\phantom{\mathbbm{R}^{n \times m}}} \mathbf{d}$, where $\mathbf{d}$ is a $\delta$-dimensional vector, representing the description of $\mathbf{G}$ in a certain feature space (the output space of $f$). Concretely, for the color modality two kinds of descriptions are extracted for each cell, Histogram of Oriented Gradients (HOG) and Histogram of Oriented Optical Flows (HOOF), whereas in the depth and thermal modality the Histogram of Oriented Normals (HON) and Histogram of Intensities and Oriented Gradients (HIOG) are respectively. Hence, we define a set of four different kinds of descriptions $\mathcal{D} = \{\mathrm{HOG}, \mathrm{HOOF}, \mathrm{HON}, \mathrm{HIOG}\}$. This way, for a particular cell $G_{rij}$, we extract the set of descriptions $D_{rij} = \{f^d(\mathbf{G}_{rij}^{(d)}) \;|\; \forall d \in \mathcal{D}\} = \{\mathbf{d}_{rij}^{(d)} \;|\; \forall d \in \mathcal{D}\}$.

\subsubsection{Color modality}
\label{sssec:color}

The color cue is nowadays the most popular imagery modality and has been extensively used to extract a range of different feature descriptions. Notwithstanding its simplicity and properties, it suffers from several drawbacks such as illumination changes, shadows, camouflage, among others, which may inconvenience some tasks.

\begin{figure*}[ht]
	\center
        %add desired spacing between images, e. g. ~, \quad, \qquad etc.
         %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.33\textwidth}
                \includegraphics[width=\textwidth]{opticalflow_final.eps}
                \caption{Optical flow}
                \label{fig:opticalflow}
        \end{subfigure}\,
        \begin{subfigure}[b]{0.32\textwidth}
                \includegraphics[width=\textwidth]{normals.png}
                \caption{Depth normals}
                \label{fig:normals}
        \end{subfigure}\,
        \begin{subfigure}[b]{0.33\textwidth}
                \includegraphics[width=\textwidth]{hihog.png}
                \caption{Thermal intensities \& oriented gradients}
                \label{fig:thermals}
        \end{subfigure}	
        \caption{Example of descriptors computed in a frame for the different modalities: (a) represents the motion vectors using a forward scheme, that is, the optical flow orientation gives insight into where the person is moving to in the next frame; (b) the computed surface normal vectors; and (c) the thermal intensities and thermal gradients' orientations. }\label{fig:descriptors}
\end{figure*}

\paragraph{Histogram of oriented gradients (HOG)}
%For the RGB cue, a simple implementation of HOG \cite{dalal2005histograms} is computed for each grid cell, known as detection window in the HOG context.
%thus having 4 cells per block and 8 blocks per window. 

For the RGB cue, we used a simple implementation of HOG \cite{dalal2005histograms} with a lower descriptor dimension than the original. It is computed for each grid cell, known as detection window in the HOG context. Each window is resized to a $h_\text{w}$ pixel area and divided in rectangular blocks of $h_\text{b}$ pixels, which are in turn divided into rectangular local spatial regions called cells of size $h_\text{c}$ pixels. We use RGB color space with no gamma correction. In order to compute the gradients, two Sobel kernels $\mbox{[-1 0 1]}$ in the x and y directions with no smoothing are used for each channel so as to find and take the channel with the greatest gradient magnitude for each pixel. 

%Each window is resized to a $h_\mathrm{w}^\mathrm{HOG} \times v_\mathrm{w}^\mathrm{HOG}$ pixel area and divided in rectangular blocks of $h_\mathrm{b}^\mathrm{HOG} \times v_\mathrm{b}^\mathrm{HOG}$ pixels, which are in turn divided into rectangular local spatial regions called cells of size $h_\mathrm{c}^\mathrm{HOG} \times v_\mathrm{c}^\mathrm{HOG}$ pixels.

%The gradient at point $\mathbf{p}$ of detection window is:

%\begin{gather}
%\mathcal{G}_\mathbf{p}^x = \mbox{[-1 0 1]} \ast \mathbf{C}_\mathbf{p} \label{eq:gx} \\[2ex]
%\mathcal{G}_\mathbf{p}^y = \mbox{[-1 0 1]}^\mathrm{T} \ast \mathbf{C}_\mathbf{p} \label{eq:gy}
%\end{gather}

%The gradient magnitude $\mathbf{M}$ and orientation $\mathbf{\Theta}$ of the gradient at point $\mathbf{p}$ are:

%\begin{gather}
%\mathbf{M}_{\mathbf{p}} = \sqrt{(\mathcal{G}_{\mathbf{p}}^{\mathrm{x}})^2 + (\mathcal{G}_{\mathbf{p}}^{\mathrm{y}})^2} \label{eq:magnitude}\\[2ex]
%\mathbf{\Theta}_{\mathbf{p}} = \arctan \Big(\frac{\mathcal{G}_{\mathbf{p}}^{\mathrm{y}}}{\mathcal{G}_{\mathbf{p}}^{\mathrm{x}}}\Big) \label{eq:orientation}
%\end{gather}

Gradient orientation is also computed for each pixel in the dominant channel and assigned into a $\kappa$-bin histogram over each cell using unsigned gradients such that bins are evenly spaced over $0^\circ$ and $180^\circ$. As stated in \cite{dalal2005histograms}, signed contrast is uninformative for humans due to the wide range of clothing and background colors. For each gradient vector, its contribution to the histogram is given by the vector magnitude, that is, stronger magnitudes have a bigger impact on the histogram. Owing to local variations in illumination and foreground-background contrast, gradient strengths vary over a wide range, so cells are grouped into larger, spatially connected blocks. Hence, the information of each cell is concatenated. Then, the gradient strengths are locally normalized applying the L2-norm over each block. Block overlap is not applied in this case so as to reduce the final descriptor dimensions. 

\paragraph{Histogram of oriented optical flow (HOOF)} 
Since we are working with video sequences, the color cue also allows us to obtain motion information by computing the dense optical flow and describing the distribution of the resultant vectors, known as histogram of oriented optical flow (HOOF)\cite{dalal2006human}. The optical-flow vectors of the whole image are computed using the luminance information of the image with the Gunnar Farneb\"{a}ck's algorithm \cite{farneback2003two} available in OpenCV\footnote{\url{http://code.opencv.org.}} \cite{bradski2008learning}, which is based on modeling the neighborhoods of each pixel of two consecutive frames by quadratic polynomials. It represents the image signal in the neighborhood of each pixel by a 3D surface and finds where the surface has moved in the next frame. As a result, a set of 2D vectors denoting the movement of each pixel for the horizontal and vertical directions in the compared frames is found. This implementation allows a wide range of parameterizations, which will be specified in Section \ref{sec:evaluation}.

 The resulting motion vectors, whose example is shown in Fig. \ref{fig:opticalflow}, are masked and quantized to produce weighted votes for local motion based on their magnitude, only taking into account those motion vector that fall inside the $G^\mathrm{color}$ grids. Such votes are locally accumulated into a $\nu$-bin histogram over each grid cell according to the signed ($0^\circ$ - $360^\circ$) vector orientations. In contrast to HOG, HOOF uses signed optical flow since the orientation information provides more discriminative power. 
 
 %Magnitude and orientation of a motion vector at pixel $\mathbf{p}$ are calculated as in Eq. \ref{eq:magnitude} and Eq. \ref{eq:orientation} respectively.

\subsubsection{Depth modality}
\label{sssec:depth}

The grid cells in the depth modality $G^\mathrm{depth}$ are depth dense maps represented as planar images of pixels that take depth values in millimeters, thus having 3D points in projective coordinates. From the depth representation in projective coordinates it is possible to obtain the "real world" ones by using the intrinsic parameters of the depth sensor. This conversion generates the set of 3D point cloud structures $\mathcal{P}$ in which distances among points are actual distances -- those that can be measured in the real world. 

After having the former conversion done, for each particular point cloud $\mathcal{P}_{rij}$ (representing an arbitrary grid cell $\mathbf{G}_{rij}$) the surface normals are computed and their distribution of angles summarized in a $\delta$-bin histogram, eventually describing the cell from the depth modality point of view.

\paragraph{Histogram of oriented depth normals (HON)} 
In order to describe an arbitrary point cloud $\mathcal{P}_{rij}$ the surface normal vector for each 3D point have to be computed first. A surface normal of a 3D point is a perpendicular vector to a 3D plane which is tangent to the surface in that point. Thus, the normal 3D vector at a given point $\mathbf{p} = (p_x, p_y, p_z) \in \mathcal{P}$ can be seen as the problem of determining the normal of a 3D plane tangent to $\mathbf{p}$. A plane is represented by the origin point $\mathbf{q}$ and the normal vector $\mathbf{n}$. From the neighboring points $\mathcal{K}$ of $\mathbf{p} \in \mathcal{P}$, we first set $\mathbf{q}$ to be the average of those points:

\begin{gather}
	\mathbf{q} \equiv \bar{\mathbf{p}} = \frac{1}{|\mathcal{K}|} \sum_{\mathbf{p} \in \mathcal{K}} \mathbf{p}.
\end{gather}
 
Then, the solution of $\mathbf{n}$ can be approximated using the covariance matrix $C \in \mathbb{R}^{3 \times 3}$ of the points in $\mathcal{P}_\mathbf{p}^{\mathcal{K}}$. The covariance matrix $C$ is computed as follows: 

\begin{gather}
	C = \frac{1}{|\mathcal{K}|} \sum_{\mathbf{p} \in \mathcal{K}} (\mathbf{p} - \bar{\mathbf{p}}) (\mathbf{p} - \bar{\mathbf{p}})^{\mathrm{T}},
\end{gather}
being $C$ a symmetric positive semi-definite matrix. Solving the next equation by means of eigenvalue decomposition:

\begin{gather}
	C \mathbf{u}_j = \lambda_j \mathbf{u}_j, \; j \in \{0,1,2\},
\end{gather}
where $\lambda_j \in \mathbb{R}$ and $\mathbf{u}_j \in \mathbb{R}^3$ represent the $j$-th eigenvalue and eigenvector of $C$ respectively. A solution for $\mathbf{n}$ is found to be the eigenvector $\mathbf{u}_j$ with the associated smaller $\lambda_j$. Formally:

% fix argmin
\begin{gather}
	\mathbf{n} = \mathbf{u}_s, \;\; \mathrm{where}\;  s = \argmin_{j}{\lambda_j},\; j \in \{0,1,2\}.
\end{gather}

The sign of $\mathbf{n}$ can be either positive or negative, and it cannot be disambiguated from the calculations. We adopt the convention of re-orienting consistently all the computed normal vectors towards the depth sensor $\mathbf{z}$ viewpoint. Moreover, a neighborhood radius parameter determines the cardinality of $\mathcal{K}$, that is, the number of points used to compute the normal vector in each of the points in $\mathcal{P}$. The computed normal vectors over a human body region is shown in Figure~\ref{fig:normals}. Points are illustrated in white, whereas normal vectors are red lines (instead of arrows for the sake of visual comprehension). The next step is to build the histogram describing the distribution of the normal vectors' orientations.

A 3D normal vector got from the previous calculations is expressed in cartesian coordinates $\mathbf{n} = (n_x, n_y, n_z)$. Nonetheless, a normal vector can be also expressed in spherical coordinates using three parameters: the radius, the inclination $\theta$, and the azimuth $\varphi$. In our case, the radius is a constant value, so this parameter can be omitted. Regarding $\theta$ and $\varphi$, the cartesian to spherical coordinates transformation is calculated as:

\begin{gather}	
	\theta  = \arctan{\left( \frac{n_z}{n_y} \right)},\;\;
	\varphi = \arccos{\frac{ \sqrt{(n_y^2 + n_z^2)} }{n_x}}.
\end{gather}

Therefore, a 3D normal vector can be characterized by a pair ($\theta$, $\varphi$) and the depth description of a cell consists of a pair of concatenated $\delta_\theta$-bin and $\delta_\varphi$-bin histograms, describing the two angular distributions of the body surface normals within the cell. Moreover, each of the two histograms is normalized before the concatenation, dividing by the number of elements, to end up with a relative angles frequency count.

\subsubsection{Thermal modality}
\label{sssec:thermal}

Whereas neither raw values of color intensity nor depth values of a pixel provide especially meaningful information for the human detection task, raw values of thermal intensity are on their own much more informative. Despite we cannot ensure human presence just because of large thermal intensity readings -- since we usually find many non-human entities, such as animals or unanimated objects, that emit a considerable amount of heat in cluttered scenes --, relatively low thermal intensities are very likely to imply the absence of human presence, That, in our case, yields to the classification of that pixel as background category. And hence, in this context of human-background pixel classification, we consider this "human heat" prior a valuable piece of information that used together with the thermal gradients, and lately fused with other cues, enhances the overall performance of our method.

\paragraph{Histogram of thermal intensities and oriented gradients (HIOG)} 
The descriptor obtained from a cell in the thermal cue $\mathbf{G}_{rij}^{\mathrm{thermal}}$ is the concatenation of two histograms. The first one is a histogram summarizing the thermal intensities, which range in the interval $[0, 255]$. The intensities are the ones in the masked region of the cell, i.e. not taking into account the background pixels. In turn, the second histogram summarizes the orientations of thermal gradients. Such gradients are computed convolving a first derivative kernel in both directions, as done previously for HOG descriptor. Then, their orientation is calculated and binned in the histogram weighted by their magnitude. Finally, the two histograms are normalized dividing by the sum of the accumulations in the bins and concatenated. We used $\alpha_{\mathrm{i}}$ bins for the intensities part and $\alpha_{\mathrm{g}}$ bins for the gradients' orientations.

%(as in Eq.~\ref{eq:gx}-\ref{eq:gy})
% TODO: specify the value of \alpha in the experiments

\subsection{Cell classification}
Since we are intended to segment human body regions, we need to distinguish those from the other foreground regions segmented by the background subtraction algorithm. These other foreground regions, apart from subjects, are the objects -- they could be other living beings, e.g. cats or dogs, although those are not considered in this work.

From the previous step, each grid cell has been described using the different descriptors $\mathcal{D}$. For the purpose of classification, we train a Gaussian Mixture Model for each grid position $(i,j)$ and type of description $d \in \mathcal{D}$. Thus, the set of all the modeled GMMs $\mathcal{M} = \{\mathcal{M}_{ij}^{(d)} \;|\; \forall i \in \{1,...,n\}, \forall j \in \{1,...,m\}, \forall d \in \mathcal{D} \}$ is obtained.

Then, in order to predict a new unseen cell $\mathbf{G}$ to be either a subject or an object, the log-likelihood value is evaluated in the probability density functions (PDFs) modeled for the mixtures of the different descriptions. We denote the log-likelihood values got by a particular cell $\mathbf{G}_{rij}$ in its corresponding GMMs $\{\mathcal{M}_{ij}^{(d)} \;|\; \forall d \in \mathcal{D}\}$ by $\{\ell_{ij}^{(d)} \;|\; \forall d \in \mathcal{D}\}$. The category -- either subject or object -- according to $d$ is predicted by comparing the log-likelihood $\ell_{rij}^{(d)}$ (got by $\mathbf{G}_{rij}$ in $\mathcal{M}_{ij}^{(d)}$) to an experimentally selected threshold value $\tau_{ij}^{(d)}$. 

Since, for each cell, $|\mathcal{D}|$ predictions are obtained, an strategy to fuse them has to be developed. Some approaches are described below.

%proposed in Section~\ref{ssec:fusion}, but first some details about the log-likelihoods computation in Gaussian Mixture Models are given below.

%\subsubsection{Gaussian Mixture Models} \label{section:gmm}
%
%Gaussian Mixture Models is an unsupervised learning method for fitting multiple Gaussians to a set of multi-dimensional data points\footnote{This technique uses properties of gaussians, thus its generalization to fit other functions is not straightforward.}. It is often used as a probabilistic clustering and as an alternative method to the k-means algorithm. As in the case of k-means, the number of components $K$ (or gaussians) in the mixture is a parameter that needs to be specified to the algorithm. The GMMs are trained using the very general Expectation-Maximization algorithm~\cite{moon1996expectation}. The goal is to end up maximizing the overall likelihood $\mathcal{L}$ of the model:
%
%\begin{gather}
%	\mathcal{L} = \prod_{\mathbf{a} \in \mathbf{A}} p(\mathbf{a})
%\end{gather}
%where $\mathbf{a}$ is a multi-dimensional data point (in this case representing the descriptor of an arbitrary grid cell $\mathbf{G}_{rij}^{d}$) belonging to the set $\mathbf{A}$, and $p(\mathbf{a})$ is the probability of $\mathbf{a}$ being drawn from the model. This probability is the value assigned by the mixture PDF to that point, which is in fact a linear combination of $K$ gaussian PDFs:
%
%\begin{gather}
%	p(\mathbf{a}) = \prod_{k=1}^{K} p(\mathbf{a}|k) P(k)
%\end{gather}
%being $p(\mathbf{a}|k)$ the value assigned by the $k$-th gaussian PDF to $\mathbf{a}$ (the height of the PDF function at that point), whereas $P(k)$ is the importance, or weight, of the $k$-th component in the mixture. In fact, since the model is a mixture of gaussians, $p(\mathbf{a})$ can be expressed as the mixture of parametrized gaussian functions:
%
%\begin{gather}
%	p(\mathbf{a}) = \prod_{k=1}^{K} \mathcal{N}(\mathbf{a}|\boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}) P(k), \\
%	\mathcal{N}(\mathbf{a}|\boldsymbol{\mu},\mathbf{\Sigma}) = \frac{1}{2\pi^{\rho/2}|\mathbf{\Sigma}|^{1/2}} \exp^{-\frac{1}{2} (\mathbf{a}-\boldsymbol{\mu})^\mathrm{T} \mathbf{\Sigma}^{-1} (\mathbf{a}-\boldsymbol{\mu}) }
%\end{gather}
%
%In order to be able to predict at some point new given examples, a training procedure is needed to model the parameters of the mixture, i.e. the means $\boldsymbol{\mu} = \{ \mathbf{\boldsymbol{\mu}}_1, ..., \boldsymbol{\mu}_K \}$ and covariances matrices $\mathbf{\Sigma} = \{ \mathbf{\Sigma}_1, ..., \mathbf{\Sigma}_K \}$. This is done by the two-step procedure called Expectaction-Maximization.
%
%\subsubsection{Expectation-Maximization: modeling a GMM}
%
%%Let be $\mathbf{A}$ the set of $\rho$-dimensional points and have initialized the parameters for the $K$ components $\boldsymbol{\mu}$ and $\mathbf{\Sigma}$ and the contribution of the components $\{P(k_1), ..., P(k_K)\}$. 
%
%The first step to perform is the expectation calculation, or \emph{E-Step}, that consists on computing the $K$ posteriors for all the points $\mathbf{a} \in \mathbf{A}$.  A typical initialization is to start with $K$ randomly chosen data points as starting means, and equal covariance matrices. Nonetheless, convergence is sometimes slow on account of having many points laying in "plateaus". Another possibility, as it has been done in this work, is to use \emph{k-means} to have a better initialization due to a more robust estimate of the initial parameters, thus increasing the convergence speed and the chances of finding a better solution. The posterior $P(k|\mathbf{a})$ is the probability the point $\mathbf{a}$ belongs to the component $k$, and it is exactly:
%
%\begin{gather}
%	p_{k\mathbf{a}} = P(k|\mathbf{a}) = \frac{\mathcal{N}(\mathbf{a}|\boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}) P(k)}{p(\mathbf{a})}
%\end{gather}
%
%Next, it is the turn of the maximization step, or \emph{M-step}. In this step, it is supposed that the assignments of individual points are known but not the model. The parameters of the components and their weights are re-estimated -- because of the previous calculations -- as:
%
%\begin{gather}
%	\hat{\boldsymbol{\mu}}_k = \frac{\sum_{\mathbf{a} \in \mathbf{A}} p_{k\mathbf{a}} \, \mathbf{a}}{\sum_{\mathbf{a} \in \mathbf{A}} p_{k\mathbf{a}}} \\
%	~
%	\hat{\mathbf{\Sigma}}_k = \frac{\sum_{\mathbf{a} \in \mathbf{A}} p_{k\mathbf{a}} \, (\mathbf{a}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{a} - \hat{\boldsymbol{\mu}}_k)^{\mathrm{T}} }{\sum_{\mathbf{a} \in \mathbf{A}} p_{k\mathbf{a}}}\\
%	~
%	\hat{P}(k) = \frac{1}{|\mathbf{A}|} \sum_{\mathbf{a} \in \mathbf{A}} p_{k\mathbf{a}}
%\end{gather}
%
%It can be proven that alternating E and M steps, the algorithm converges to at least a local maximum of overall likelihoods. Moreover, though not explained, dealing with likelihoods may cause underflow problems in the computations. The approach to cope with this problem is to apply logarithms, that is dealing with log-likelihoods instead of likelihoods. Despite this changes some calculations re-formulated using the so-called "log-sum-exp" trick, the EM algorithm is still a valid approach to maximize the log-likelihood of the model given $\mathbf{A}$.
%
\subsection{Multi-modal fusion} 
\label{ssec:fusion}

Our hypothesis is that the fusion of different modalities and descriptions -- which provide a more informative and richer representation of the scenario -- can improve the final segmentation result. Such fusion can be achieved using several approaches, which are detailed below.
 
Nonetheless, before fusing the results got from the different descriptions, a normalization step is required. This normalization comprises centering and scaling the log-likelihood values corresponding to the predictions, since they are got from the GMMs of the different descriptions. More precisely, the log-likelihood values are centered around $\tau_{ij}^{d}$ and scaled dividing this centered value by a deviation-like term. This deviation term is simply the mean squared difference in the sample respect to $\tau_{ij}^{d}$. In such a way, the normalized log-likelihood $\varrho$ of the two categories (object and subject) conveniently differ in their sign.

\subsubsection{Description-level predictions}
\label{sssec:descriptionlevelpredictions}

Prior to the fusion step, it may be necessary to provide a prediction at grid-level (instead of cell-level) for each of the four descriptions individually. For this reason, a strategy is needed to achieve the predictions consensus of the grid cells. 

The consensus is attained by a voting scheme. In case of draw the magnitude of the mean normalized log-likelihoods obtained for both categories are compared; since normalized log-likelihood values $\varrho$ are centered at the decision threshold $\tau$, $\varrho$, which is in fact a distance to the predictor margin, can be interpreted as a \textit{confidence} value. Thus, a grid consensus function $g(\cdot)$ is defined as follows:

\begin{gather*}\tiny
v_r^{(0)} = \sum_{i,j} \mathbbm{1}\{\varrho_{rij} < 0\} \;,\;\; v_r^{(1)} = \sum_{i,j} \mathbbm{1}\{\varrho_{rij} > 0\}  \\
\varrho_{r}^{(0)} = \sum_{(i,j) \,|\, \varrho_{rij} < 0} \varrho_{rij} \;,\;\; \varrho_{r}^{(1)} = \sum_{(i,j) \,|\, \varrho_{rij} > 0} \varrho_{rij} \\
g(r) =
\left\{
	\begin{array}{ll}
		0  & \mbox{if }  v_r^{(0)} > v_r^{(1)} \\
		\mathbbm{1}\left\{ |\varrho_{r}^{(0)}| < |\varrho_{r}^{(1)}| \right\} &  \mbox{if } v_r^{(0)} = v_r^{(1)} \\
		1 & \mbox{if }  v_r^{(0)} < v_r^{(1)} \\
	\end{array},
\right.
\end{gather*}
where $v_r^{(0)}$ and $v_r^{(1)}$ keep count of the votes of the $r$ grid cells for object (negative normalized log-likelihood) and subject (positive normalized log-likelihood) respectively. $\varrho_r^{(0)}$ and $\varrho_r^{(1)}$ are the accumulations of negative and positive normalized log-likelihoods respectively. Eventually, the consensus log-likelihood $\hat{\varrho}$ is also computed for further usage:

\begin{gather*}\scriptsize
\hat{\varrho}_r =
\left\{
	\begin{array}{ll}
		\frac{1}{nm / 2} \, \varrho_{r}^{(0)}  & \mbox{if }  g(r) = 0  \\
		\frac{1}{nm / 2} \, \varrho_{r}^{(1)} & \mbox{if }  g(r) = 1
	\end{array}
\right.
\end{gather*}

From this calculations, it is determined the category of a grid $r$ from each of the description's point of view and the associated consensus log-likelihoods. Once computed, the next step is to establish how their are fused to provide the final category of $r$.

\subsubsection{Heuristic fusion approaches}

The heuristic approaches for the fusion of the predictions from the different descriptors has been analogously defined to the grid cells consensus strategy. We defined three different approaches to tackle the fusion problem in an heuristic-like manner.

\paragraph{Grid cells pre-consensus}

In the grid cells pre-consensus strategy, the categories determined from the different descriptions' point of view in Section~\ref{sssec:descriptionlevelpredictions} are directly used to perform another voting. Draws are dealt by using the consensus log-likelihoods. In this case, votes are obtained from the different descriptions, not the cells. A fusion function $\mathcal{F}(\cdot)$ is defined as follows:

\begin{gather*}\tiny
\vartheta_r^{(0)} = \sum_{d} \mathbbm{1}\{ g^{(d)}(r) = 0 \} \; , \;\; \vartheta_r^{(1)} = \sum_{d} \mathbbm{1}\{ g^{(d)}(r) = 1 \} \\
\hat{\varrho}_r^{(0)} = \sum_{d \,|\, g^{(d)}(r) = 0} \hat{\varrho}_r^{(d)} \; , \;\; \hat{\varrho}_r^{(1)}  =\sum_{d \,|\, g^{(d)}(r) = 1} \hat{\varrho}_r^{(d)} \\
\mathcal{F}(r) =
\left\{
	\begin{array}{ll}
		0  &  \mbox{if } \vartheta_r^{(0)} < \vartheta_r^{(1)}   \\
		\mathbbm{1}\left\{ | \hat{\varrho}_r^{(0)} | < | \hat{\varrho}_r^{(1)} | \right\} &  \mbox{if } \vartheta_r^{(0)} = \vartheta_r^{(1)}   \\
		1 &  \mbox{if } \vartheta_r^{(0)} > \vartheta_r^{(1)}   \\
	\end{array},
\right.
\end{gather*}
where $\vartheta_r^{(0)}$ and $\vartheta_r^{(1)}$ count the votes of the descriptions for object (0 category in the cells consensus function $g(\cdot)$) and subject (1 category in the cells'consensus function $g(\cdot)$) and $\hat{\varrho}_r^{(0)}$ respectively. $\hat{\varrho}_r^{(1)}$ are the accumulations of negative and positive consensus log-likelihoods respectively.

\paragraph{Grid cells post-consensus}

An alternative approach to the pre-consensus fusion is simply performing the fusion at cell-level by voting first and, then, conducting another voting to achieve the cells consensus.

%\begin{gather*}\tiny
%\vartheta_{rij}^{(0)} = \sum_{d} \mathbbm{1}\{ \ell_{rij}^d < 0 \} \; , \;\; \vartheta_{rij}^{(1)} = \sum_{d} \mathbbm{1}\{ \ell_{rij}^d > 0 \} \\
%\ell_{rij}^{(0)} = \sum_{d \,|\, \ell_{rij}^d < 0} \ell_{rij}^d \;,\;\; \ell_{rij}^{(1)} = \sum_{d \,|\, \ell_{rij}^d > 0} \ell_{rij}^d \\
%h(r,i,j) =
%\left\{
%	\begin{array}{ll}
%		0  & \mbox{if }  \vartheta_{rij}^{(0)} > \vartheta_{rij}^{(1)} \\
%		\mathbbm{1}\left\{ |\ell_{rij}^{(0)}| < |\ell_{rij}^{(1)}| \right\} &  \mbox{if }  \vartheta_{rij}^{(0)} = \vartheta_{rij}^{(1)} \\
%		1 & \mbox{if }  \vartheta_{rij}^{(0)} < \vartheta_{rij}^{(1)} \\
%	\end{array}
%\right.\\
%\hat{\ell}_{rij} =
%\left\{
%	\begin{array}{ll}
%		\frac{1}{|\mathcal{D}| / 2} \, \ell_{rij}^{(0)}  & \mbox{if }  g(r) = 0  \\
%		\frac{1}{|\mathcal{D}| / 2} \, \ell_{rij}^{(1)} & \mbox{if }  g(r) = 1
%	\end{array}
%\right.\\
%v_r^{(0)} = \sum_{i,j} \mathbbm{1}\{h(r,i,j) = 0\} \;,\;\; v_r^{(1)} = \sum_{i,j} \mathbbm{1}\{h(r,i,j) = 1\}  \\
%\ell_{r}^{(0)} = \sum_{(i,j) \,|\, h(r,i,j) = 0} \ell_{rij}^{(0)} \;,\;\; \ell_{r}^{(1)} = \sum_{(i,j) \,|\, h(r,i,j) = 1} \ell_{rij}^{(1)} \\
%\mathcal{G}(r) =
%\left\{
%	\begin{array}{ll}
%		0  &  \mbox{if } \vartheta_r^{(0)} < \vartheta_r^{(1)}   \\
%		\mathbbm{1}\left\{ | \hat{\ell}_r^{(0)} | < | \hat{\ell}_r^{(1)} | \right\} &  \mbox{if } \vartheta_r^{(0)} = \vartheta_r^{(1)}   \\
%		1 &  \mbox{if } \vartheta_r^{(0)} > \vartheta_r^{(1)}   \\
%	\end{array}
%\right.
%\end{gather*}


\paragraph{Distance-based fusion and cells post-consensus}

The difference between this last approach and the former one is the way to fuse the information at description-level. While the former determines the category at cell-level by voting, not taking into account the normalized log-likelihood value but the sign (except for draw cases), this later approach uses as a basis the magnitude of the normalized log-likelihood values in the descriptions fusion process. As in \textit{cells post-consensus} the grid cells decisions are consensued by voting.

The main drawback of heuristic fusion approaches is the impossibility to determine the importance of each of the description-level predictions when fusing or the relevance of the cell predictions when performing the grid cells consensus. For instance, the thermal cue could be providing the most reliable description, or the grid cells comprising the upper body could provide more reliable results than the lower body ones. For this reason, we propose the next approach, which consists in using state-of-the-art statistical learning algorithms to get grid category predictions somehow from \textit{stacked} GMMs' predictions.

\subsubsection{Learning-based fusion approach}

As before, the category of a grid $r$ is intended to be predicted, but instead of directly making use of the descriptors computed from the different modalities, we exploit the predictions got from the GMMs in different cells and each type of description. This approach follows the Stacked Learning scheme \cite{cohen2005stacked, puertas2013generalized}, which involves training a new learning algorithm combining previous predictions obtained with other learning algorithms. More precisely, each grid $r$ is represented by a vector $\mathbf{v}_r$ of normalized log-likelihoods:

$$\mathbf{v}_r = (\varrho_{r11}^{(1)}, ..., \varrho_{rNM}^{(1)}, ..., \varrho_{r11}^{(|\mathcal{D}|)}, ..., \varrho_{rNM}^{(|\mathcal{D}|)}, y_r) ,$$
where $y_r$ is the actual category of the $r$ grid. Using such representation of the normalized log-likelihoods in the different grid cells and modalities, a data sample containing the $R$ feature vectors of this kind is built. This way, any supervised learning algorithm can be used to try to learn from this data and infer more reliable predictions than using the previous handcrafted heuristics. 

Among many existing state-of-the-art supervised learning algorithms for classification, we have selected the following ones: Adaptive Boosting (gentle version)~\cite{freund1999short}, Artificial Neural Networks (Multi-Layer Perceptron~\cite{rosenblatt1961principles} with both sigmoidal and radial basis activation function), and Support Vector Machines~\cite{cortes1995support} (linear and radial basis function kernels).

%\paragraph{Adaptive Boosting (AdaBoost)} is a learning meta-algorithm that builds a strong classifiers by combining many weak classifiers. This combination can be a simple weighted sum of the weak classifiers' outputs. The idea is the aggregation weak predictions (barely better than random guessing) boosts the strong decision. Decision trees are the most popular weak classifiers used in boosting schemes, ...
%
%\paragraph{Multi-Layer Perceptron (MLP)} is a feed-forward artificial neural network model, graph-like represented, which maps a multi-dimensional input to the proper multi-dimensional output~\cite{rosenblatt1961perceptrons}. A MLP consists of more than two fully connected layers of nodes (or neurons). Particularly, the MLP aggregates and stacks many perceptrons in order to, respectively, deal with the output multi-dimensionality and the non-linear separability of the data. Each layer is composed of neurons (except for the input layer), each of them fully connected to the next one; that is, a given neuron receives the input of all the neurons of the previous layer's neurons and in turn this neuron emits a synchronous signal directed to each of the neurons in the next layer. The training of an artificial neural network model consists in setting the weights in the neural connections so as to maximize the prediction performance, and it is typically done with the so-called backpropagation algorithm~\cite{rumelhart1986learning}; this technique iteratively inputs data to the network and backpropagates the error got in the output, adapting somehow the weights so as to minimize the error the output error is minimized after each iteration.
%
%While the number of neurons in the hidden layer $h$ is a parameter that has to be selected, the number of neurons in both the input layer and output layer are determined, respectively, by the input data dimensionality and the number of output variables that need to be predicted (number of classes in classification problems). Other model parameters are the number of epochs $e$ (iterations of error backpropagation), and the weight update factor $\alpha$.
%
%% TODO: add to the bib: rosenblatt1961perceptrons, rumelhart1986learning
%
%\paragraph{Support Vector Machine (SVM)} is a non-probabilistic supervised binary classifier that learns a model which represents the instances as points in space, mapped in such a way that instances of different classes are separated by a hyperplane in a high dimensional space. However, if the dataset is not linearly separable in that space the hyperplane will fail in classifying properly. This can be solved by mapping the dataset instances into a higher dimensional space using a kernel function, thus making easier the dataset division \cite{hearst1998support}. 
%
%In particular, our baseline includes linear kernel and radial basis function (RBF)\footnote{The SVM models has been trained using the available implementation of the LibSVM\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/ }} library \cite{chang2011libsvm}.}. Linear SVM just requires one parameter, the penalty parameter $\zeta$, which specifies a trade-off between model complexity and misclassification of training examples and can take values in the interval $[0, inf)$. Higher values of $\zeta$ causes closer fitting to the training set, which may tend to overfitting. The performance of the RBF kernel is also influenced by the $\gamma$ parameter, which controls the shape of the separating hyperplane. Higher values usually increase the number of support vectors. Weights $\mathbf{w}$ obtained from linear SVM represent the hyperplane used to separate between classes, but they can also give us an insight of the level of importance or level of influence that each feature has when classifying the instance \cite{guyon2003introduction}.

%Such parameters have influence on the model accuracy, for that reason a proper parameter selection has to be performed

\section{Evaluation}
\label{sec:evaluation}
The former approaches are used to eventually predict the final binary masks which aim to discard non-subject regions. We test our approach on the aforementioned tri-modal dataset, and evaluate the obtained results in terms of both per-region detection accuracy and per-frame overlap. The evaluation parameters and experimental procedures are detailed below. 

\subsection{Parameters and settings}
\label{ssec:parametersandsettings}

%After some experiments regarding the use of Otsu's threshold in the background subtraction and generation of bounding boxes stage, 
We experimentally set $\sigma_\text{otsu} = 8.3$ for a connected component area of at least 0.1\% of the image, and  $\sigma_\text{otsu} = 12$ for other cases. This settings were established in order to maintain a trade-off between finding the maximum number of overlapping people situations without dividing a subject in different regions depending on the variation of depth of the body parts.

Since it is not possible to have a pixel-to-pixel correspondence among modalities, we define the correspondence at a grid cell level. The grids have been partitioned in $m \times n$ cells, being $m = 2$ and $n = 2$. The main idea of the grid partitioning is to reduce the variability of the regions in each GMM. At the same time, they are large enough to not condition the eventually computed overlap measure.

For the HOG descriptor our purpose was to have a low dimensional - lower than the original - yet descriptive feature vector, so we defined $h_\text{w} = 64 \times 128$, $h_\text{b} = 32 \times 32$, $h_\text{c}=16 \times 16$, and $\kappa = 9$. The information of each cell is concatenated resulting in a vector of 36 values per block. This brings the final vector size of a grid cell to 4 blocks vertically $\times$ 2 blocks horizontally $\times$ 4 cells per block $\times$ 9 bins per cell, making a total of 288 components/dimensions. 

In order to compute the optical flow, and based on the tests performed in \cite{brkic2013combining}, we set the parameters of the given implementation according to the values that gave the best performance. In particular, the averaging window size was set to 2, the size of the pixel neighborhood considered when finding polynomial expansion in each pixel was set to 5 and the standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion to 1.1.  The remaining parameters were set to their default OpenCV values. For the motion descriptor (HOOF), we defined $\nu = 8$ to finally produce an 8-dimensional feature vector. 

Then, for the depth descriptors (HON), we defined $\delta_\theta = 8$ and $\delta_\varphi = 8$, whereas for the thermal descriptors (HIOG), we defined $\upsilon_{\mathrm{i}} = 8$ and $\upsilon_{\mathrm{g}} = 8$. A very large number of bins is not convenient for generalization. In addition, we wanted the number of bins describing orientations in the different descriptions to coincide for comparison purposes in later experiments; and 8 bins for describing the orientations of the normal vectors seem a quite reasonable amount of bins in view of the depth sensor noisy measurements. In fact, to deal with this noise the neighborhood radius in the normal computation needs to be adjusted. Thus, we empirically selected between two values ($\{0.02, 0.04\}$) the one performing better (according to our validation measures explained next), that in this case has been 0.02. That demonstrated that larger values would be providing worse performances, while smaller ones are not reasonable as they affect the orientations.

Other more sensitive parameters, especially the ones related to learning algorithms, have been experimentally selected. In the GMM-related experiments, $k = \{2,3, ..., 9, 11, 13\}$ and $\tau = (-2,2)$ (in 32 equal width steps). In the AdaBoost experiment, the number of possible weak classifiers and the weight trimming rates $\{10, 20, 50, 100, 200, 500, 1000\}$ and $\{0, 0.7, 0.75, 0.8, \ldots, 1\}$ respectively; in the MLP, the number of neurons of the hidden layer was set to $h = \{2, 5, 10, 15, \ldots, 50, 60, 70, \ldots, 100\}$; and, in the SVM, $\zeta = \{1e-7, 1e-6, \ldots, 1e4\}$ and $\gamma = \{1e-7, 1e-6, \ldots, 1e2\}$.



\subsection{Experimental methodology and validation measures}
\label{ssec:validation}

The proposed baseline has been validated by means of cross-validation (CV). For this purpose, we divided the dataset at frame-level in 10 non-overlapping partitions, with each partition containing perhaps a slightly different number of frames but the same number of subject regions. This ensures a fair training of the models across different folds, i.e. training the models with the same number of subject examples. In order to do that, we simply stratified the partitions by the number of appearing subject regions in the associated ground-truth masks. The dataset could not be directly partitioned at region-level, since the performance of our method is eventually evaluated by a per-frame overlap measure.

%because a per-frame overlap measures the performance of our method.

In addition, we performed a model selection in each training partition so as to find the optimal values for the GMMs experimental parameters ($k$ and $\tau$). Again, we used the cross-validation strategy, but with a smaller number of partitions (3). In each inner fold, a grid search has been carried out to measure the performance of each $k$-$\tau$ combination. The optimal combination, that is, the one performing better in average across these inner folds, was used to train the final models eventually validated in the test partitions. In this case, and since we measured the subject prediction accuracy, the data was partitioned at region-level.

The parameters of the learning-based fusion classifiers were also selected for each training partition of the outer CV. This is done in the exact same inner 3-fold cross-validation partitions. While the selection of $k$ and $\tau$ was exhaustive enough, the parameters involved in these supervised learning algorithms often require an even more exhaustive search to fine tune their values. A common strategy to keep the number of parameter combinations manageable is to perform a two-level grid search, which consists of a first coarse grid search followed by a second narrow grid search around the coarse optimal values.

For the sake of completeness, we included in some of our experiments the mirrored versions of the regions of interest along a vertical axis to train the model, along with the original ones.  At test stage, only original regions of interest were used. 

% A table of results here

As previously mentioned, we used an overlap measure in order to compare the similarity between the ground-truth masks and the predicted masks. Concretely, the Jaccard Index \cite{tan2002selecting}, also known as the Jaccard similarity coefficient. The degree of overlap between two binary sets $A$ and $B$ is computed as the ratio between the size of the intersection divided by the size of the union:
\begin{equation} \label{eq:jaccard}
overlap(A, B) = \frac{|A \cap B|}{|A \cup B|}.
\end{equation}

This measure takes values in $[0,1]$, 0 meaning no overlap and 1 meaning perfect agreement between sets. For each frame, connected components of the ground-truth masks are compared to the predicted binary masks from the different modalities individually or from the different fusion approaches. The overlap is computed per person id and connected component, in such a way that connected components that have the same person id or are connected in the ground truth constitute a set $A$, and they are compared to the blobs that coincide in the same coordinates in the predicted binary masks, which constitute a set $B$. The overlap of each frame is then averaged by the number of sets found. It is therefore a pessimistic measure because a very tiny blob misclassified as a person in the predicted binary masks will account for 0 overlap, thus decreasing the mean overlap of the frame, so it can be considered as a lower bound on how accurate the prediction is. The final overlap is computed as the mean overlap of all frames of each fold having at least one blob, whether it be in the ground truth or in the predicted binary mask.

As commented in Section~\ref{sect:bs}, the depth cue suffers from a halo effect around people or objects, thus complicating an accurate pixel-level segmentation at blob contours when applying background subtraction. This lack of accuracy is also caused by possible distortions, noise or other problems, and decreases the final overlap. Hence, a \emph{do not care region} (DCR) is often used. Such region is taken per frame by centering a morphology operator of different sizes at the ground-truth binary masks blob contours and subtracting it from those masks and from the predicted ones to compute the overlap. This way, we can compare the effect of using a growing DCR to the actual overlap.

%As explained in the last section, we assess the performance of the proposed baseline using both a detection accuracy and overlap measure (Eq. \ref{eq:jaccard}). 

\subsection{Experiments}

The performance of the proposed baseline is assessed using both detection accuracy and overlap measures. We test it both including the mirrored regions at training step and without including them. This way, we can compare the possible differences in terms of both measures.
The obtained results are presented below.


%detection accuracy and segmentation overlap. 

%First, we illustrate the results obtained in the subject detection accuracy experiments with and without mirroring in Table~\ref{tab:subject_detection_accuracy} and Table ~\ref{tab:subject_detection_accuracy} respectively. After that, segmentation overlap experiments with and without mirroring are detailed.

\subsubsection{Experiment 1: Subject detection}

Table~\ref{tab:subject_detection_accuracy} illustrates the results obtained in the subject detection accuracy experiments with and without mirroring. Note we achieve nearly perfect  detection when we only consider the two categories, 0 (object) and 1 (subject), omitting the -1 category prediction. Thermal description is the one that achieves the best accuracy results at the description-level prediction approach, substantially improving the usual color description by 35\%. There is no significant difference among the proposed heuristic fusion approaches, which perform slightly worse than the individual thermal description. However, such description is outperformed by our learning-based fusion approaches. More precisely, we achieve the bests results using SVM with RBF kernel, increasing the individual thermal description accuracy by 7\%.

\begin{table}[ht]
\caption{Subject detection accuracy}
\begin{center}
\begin{tabular}{lll}
    \hline
    Prediction approach & & Accuracy $\pm$ CI \\
    \hline
    \multirow{2}{*}{Description-level} & Color & 54.17\% $\pm$ 0.77 \\
    & Motion & 78.69\% $\pm$ 1.12\\
    & Depth & 78.63\% $\pm$ 1.94\\
    & Motion & 89.10\% $\pm$ 0.70\\
    \hline
    \multirow{2}{*}{Heuristic fusion} & Pre-consensus & 86.61\% $\pm$ 1.05 \\
    & Pos-consensus & 88.23\% $\pm$ 0.84 \\
    & Distance-based & 88.19\% $\pm$ 0.75 \\
    \hline
    \multirow{2}{*}{Learning-based fusion} & AdaBoost & 94.97\% $\pm$ 1.14 \\
    & MLP (sigmoid) & 95.68\% $\pm$ 0.70 \\
    & MLP (gaussian) & 94.94\% $\pm$ 2.98 \\
    & SVM (linear) & 91.96\% $\pm$ 2.03 \\
    & SVM (rbf) & 96.03\% $\pm$ 0.99 \\
    \hline
\end{tabular}
\end{center}
\label{tab:subject_detection_accuracy}
\end{table}

\subsubsection{Experiment 2: Subject segmentation }

Tables \ref{tab:individual}, \ref{tab:simplefusion} and \ref{tab:learningfusion} show the obtained overlap for individual and fusion predictions with different fusion approaches. Notice that in tables showing fusion results, only two cases are considered, since color and depth modalities share the same original foreground masks. Finally, Figure \ref{fig:results} shows some examples of final results comparing the initial masks extracted from background subtraction to the final predicted masks obtained with RBF SVM, which is the one that gives the best overlap results.

\begin{table*}[ht]
\caption{Overlap and confidence interval results of the different modalities individually depending on the DCR used}
\begin{center}
\begin{tabular}{c|c|c|c|c}
    \hline
    DCR & Color & Motion & Depth & Thermal \\
    \hline
    -  & 36.52\% $\pm$ 1.86 & 54.53\% $\pm$ 0.70 & 44.44\% $\pm$ 1.87 & 43.78\% $\pm$ 0.28 \\
    3 & 38.20\% $\pm$  1.95 & 57.12\% $\pm$ 0.74 & 46.66\% $\pm$ 1.94 & 45.54\% $\pm$ 0.29 \\
    5 & 39.28\% $\pm$ 2.02 & 58.95\% $\pm$ 0.77 & 48.22\% $\pm$ 1.99 & 47.19\% $\pm$ 0.31 \\
    7 & 40.21\% $\pm$ 2.06 & 60.26\% $\pm$ 0.78 & 49.35\% $\pm$ 2.02 & 48.69\% $\pm$ 0.32 \\
    9 & 40.81\% $\pm$ 2.09 & 61.18\% $\pm$ 0.79 & 50.16\% $\pm$ 2.05 & 50.01\% $\pm$ 0.33 \\
    11 & 41.22\% $\pm$ 2.11 & 61.79\% $\pm$ 0.80 & 50.73\% $\pm$ 2.06 & 51.17\% $\pm$ 0.35 \\
    13 & 41.45\% $\pm$ 2.12 & 62.14\% $\pm$ 0.80 & 51.09\% $\pm$ 2.07 & 52.17\% $\pm$ 0.36 \\
    15 & 41.57\% $\pm$ 2.11 & 62.32\% $\pm$ 0.80 & 51.33\% $\pm$ 2.08 & 53.02\% $\pm$ 0.37 \\
    17 & 41.59\% $\pm$ 2.10 & 62.38\% $\pm$ 0.79 & 51.46\% $\pm$ 2.08 & 53.73\% $\pm$ 0.39 \\
\end{tabular}
\end{center}
\label{tab:individual}
\end{table*}

\begin{table*}[ht]
\caption{Overlap and confidence interval results of the heuristic fusion approach depending on the DCR used}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
    & \multicolumn{2}{c|}{Pre-consensus} & \multicolumn{2}{c|}{Post-consensus} & \multicolumn{2}{c}{Distance-based} \\

    \cline{1-7}
    DCR & Color/Depth & Thermal & Color/Depth & Thermal & Color/Depth & Thermal\\
    \hline
    -  & 51.69\% $\pm$ 0.68 & 42.82\% $\pm$ 0.62 & 52.63\% $\pm$ 0.53 & 43.42\% $\pm$ 0.43 & 52.80\% $\pm$ 0.53 & 43.54\% $\pm$ 0.42 \\
    3 & 54.15\% $\pm$ 0.71 & 44.54\% $\pm$ 0.64  & 55.13\% $\pm$ 0.55 & 45.16\% $\pm$ 0.45 & 55.32\% $\pm$ 0.55 & 45.29\% $\pm$ 0.44 \\
    5 & 55.87\% $\pm$ 0.73 & 46.16\% $\pm$ 0.67  & 56.89\% $\pm$ 0.57 & 46.80\% $\pm$ 0.46 & 57.08\% $\pm$ 0.57 & 46.93\% $\pm$ 0.46 \\
    7 & 57.12\% $\pm$ 0.75 & 47.64\% $\pm$ 0.69  & 58.16\% $\pm$ 0.59 & 48.30\% $\pm$ 0.48 & 58.35\% $\pm$ 0.59 & 48.44\% $\pm$ 0.47 \\
    9 & 58.03\% $\pm$ 0.76 & 48.95\% $\pm$ 0.70  & 59.07\% $\pm$ 0.59 & 49.63\% $\pm$ 0.49 & 59.28\% $\pm$ 0.59 & 49.77\% $\pm$ 0.49 \\
    11 & 58.66\% $\pm$ 0.76 & 50.12\% $\pm$ 0.72  & 59.71\% $\pm$ 0.60 & 50.80\% $\pm$ 0.51 & 59.92\% $\pm$ 0.59 & 50.95\% $\pm$ 0.49 \\
    13 & 59.06\% $\pm$ 0.76 & 51.12\% $\pm$ 0.73  & 60.11\% $\pm$ 0.61 & 51.81\% $\pm$ 0.52 & 60.32\% $\pm$ 0.60 & 51.96\% $\pm$ 0.51 \\
    15 & 59.31\% $\pm$ 0.76 & 51.97\% $\pm$ 0.74  & 60.36\% $\pm$ 0.60 & 52.67\% $\pm$ 0.54 & 60.58\% $\pm$ 0.59 & 52.83\% $\pm$ 0.52 \\
    17 & 59.44\% $\pm$ 0.76 & 52.70\% $\pm$ 0.75  & 60.49\% $\pm$ 0.60 & 53.41\% $\pm$ 0.55 & 60.72\% $\pm$ 0.59 & 53.57\% $\pm$ 0.53 \\
\end{tabular}
\end{center}
\label{tab:simplefusion}
\end{table*}

\begin{table*}[ht]
\caption{Overlap and confidence interval results of the learning-based fusion depending on the DCR used}
\begin{center}
\begin{tabular}{c|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}|C{1.2cm}}
& \multicolumn{2}{c|}{AdaBoost} & \multicolumn{2}{c|}{Gaussian MLP} & \multicolumn{2}{c|}{Sigmoid MLP} & \multicolumn{2}{c|}{Linear SVM} & \multicolumn{2}{c}{RBF SVM}  \\

\cline{1-11}
DCR & Color /\par Depth\strut & Thermal & Color /\par Depth\strut & Thermal & Color /\par Depth\strut & Thermal & Color /\par Depth\strut & Thermal & Color /\par Depth\strut & Thermal\\
\hline
-  & 59.69\% \par$\pm$ 0.29\strut & 48.68\% \par$\pm$ 0.17\strut & 58.76\% \par$\pm$ 0.50\strut & 48.05\% \par$\pm$ 0.33\strut & 58.90\% \par$\pm$ 0.47\strut & 48.15\% \par$\pm$ 0.34\strut & 59.73\% \par$\pm$ 0.43\strut & 48.76\% \par$\pm$ 0.23\strut & 59.96\% \par$\pm$ 0.25\strut & 48.85\% \par$\pm$ 0.15\strut \\
3 & 62.57\% \par$\pm$ 0.31\strut & 50.62\% \par$\pm$ 0.17\strut & 61.60\% \par$\pm$ 0.53\strut & 49.97\% \par$\pm$ 0.35\strut & 61.74\% \par$\pm$ 0.49\strut & 50.07\% \par$\pm$ 0.35\strut & 62.64\% \par$\pm$ 0.46\strut & 50.71\% \par$\pm$ 0.24\strut & 62.87\% \par$\pm$ 0.27\strut & 50.80\% \par$\pm$ 0.15\strut \\
5 & 64.63\% \par$\pm$ 0.33\strut & 52.45\% \par$\pm$ 0.18\strut & 63.61\% \par$\pm$ 0.56\strut & 51.77\% \par$\pm$ 0.36\strut & 63.76\% \par$\pm$ 0.51\strut & 51.88\% \par$\pm$ 0.36\strut & 64.71\% \par$\pm$ 0.48\strut & 52.54\% \par$\pm$ 0.25\strut & 64.94\% \par$\pm$ 0.28\strut & 52.63\% \par$\pm$ 0.16\strut \\
7 & 66.12\% \par$\pm$ 0.34\strut & 54.13\% \par$\pm$ 0.19\strut & 65.08\% \par$\pm$ 0.57\strut & 53.43\% \par$\pm$ 0.38\strut & 65.24\% \par$\pm$ 0.52\strut & 53.54\% \par$\pm$ 0.37\strut & 66.21\% \par$\pm$ 0.49\strut & 54.23\% \par$\pm$ 0.26\strut & 66.45\% \par$\pm$ 0.29\strut & 54.32\% \par$\pm$ 0.17\strut \\
9 & 67.20\% \par$\pm$ 0.34\strut & 55.61\% \par$\pm$ 0.20\strut & 66.13\% \par$\pm$ 0.58\strut & 54.90\% \par$\pm$ 0.39\strut & 66.31\% \par$\pm$ 0.52\strut & 55.02\% \par$\pm$ 0.38\strut & 67.28\% \par$\pm$ 0.51\strut & 55.72\% \par$\pm$ 0.27\strut & 67.53\% \par$\pm$ 0.30\strut & 55.81\% \par$\pm$ 0.18\strut \\
11 & 67.92\% \par$\pm$ 0.35\strut & 56.92\% \par$\pm$ 0.21\strut & 66.86\% \par$\pm$ 0.58\strut & 56.20\% \par$\pm$ 0.40\strut & 67.04\% \par$\pm$ 0.52\strut & 56.32\% \par$\pm$ 0.38\strut & 68.01\% \par$\pm$ 0.51\strut & 57.04\% \par$\pm$ 0.27\strut & 68.26\% \par$\pm$ 0.29\strut & 57.13\% \par$\pm$ 0.18\strut \\
13 & 68.35\% \par$\pm$ 0.34\strut & 58.04\% \par$\pm$ 0.21\strut & 67.30\% \par$\pm$ 0.57\strut & 57.32\% \par$\pm$ 0.40\strut & 67.50\% \par$\pm$ 0.52\strut & 57.45\% \par$\pm$ 0.38\strut & 68.43\% \par$\pm$ 0.51\strut & 58.16\% \par$\pm$ 0.27\strut & 68.69\% \par$\pm$ 0.29\strut & 58.26\% \par$\pm$ 0.18\strut \\
15 & 68.60\% \par$\pm$ 0.34\strut & 58.98\% \par$\pm$ 0.22\strut & 67.55\% \par$\pm$ 0.56\strut & 58.26\% \par$\pm$ 0.41\strut & 67.78\% \par$\pm$ 0.50\strut & 58.41\% \par$\pm$ 0.38\strut & 68.66\% \par$\pm$ 0.50\strut & 59.11\% \par$\pm$ 0.27\strut & 68.94\% \par$\pm$ 0.28\strut & 59.21\% \par$\pm$ 0.18\strut \\ 
17 &  68.69\% \par$\pm$ 0.33\strut & 59.76\% \par$\pm$ 0.22\strut & 67.67\% \par$\pm$ 0.54\strut & 59.05\% \par$\pm$ 0.40\strut & 67.93\% \par$\pm$ 0.49\strut & 59.22\% \par$\pm$ 0.37\strut & 68.73\% \par$\pm$ 0.49\strut & 59.89\% \par$\pm$ 0.26\strut & 69.03\% \par$\pm$ 0.27\strut & 60.00\% \par$\pm$ 0.18\strut \\
         
\end{tabular}
\end{center}
\label{tab:learningfusion}
\end{table*}

%
%\begin{table}[h]\footnotesize
%\center
%\caption{Overlap results of the individual predictions for each description}
%\label{table:individual}
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
%&\textbf{HOG}&\textbf{SM}&\textbf{HOOF}&\textbf{HIOG}&\textbf{HON}\\\hline
%\textbf{0}&62.10 \% &63.12 \%&56.97 \%&46.35 \%&56.76 \%\\\hline
%\textbf{1}&64.71 \%&65.85 \%&59.41 \%&47.99 \%&59.09 \%\\\hline
%\textbf{3}&67.59 \%&69.02 \%&62.13 \%&50.85 \%&61.70 \%\\\hline
%\textbf{5}&68.65 \%&70.40 \%&63.20 \%&53.02 \%&62.77 \%\\\hline
%\textbf{7}&68.65 \%&70.72 \%&63.28 \%&54.45 \%&62.94 \%\\\hline
%\end{tabular}
%\end{table}
%
%
%\begin{table}[h]\footnotesize
%\center
%\caption{Overlap results of fusion using Stacked Linear SVM for each modality}
%\label{table:linearstacked}
%\begin{tabular}{|c|c|c|}
%\hline
%\textbf{DCR}&\textbf{Thermal}&\textbf{Color/Depth}\\\hline
%\textbf{0}&49.64 \%&64.65 \%\\\hline
%\textbf{1}&51.33 \%&67.39 \%\\\hline
%\textbf{3}&54.29 \%&70.43 \%\\\hline
%\textbf{5}&56.56 \%&71.58 \%\\\hline
%\textbf{7}&58.11 \%&71.63 \%\\\hline
%\end{tabular}
%\end{table}

%Linear SVM is used as baseline classifier. It offers good performance relative to other linear classifiers and %fast to run.


% \begin{figure}[ht]
%	 \centering
%	 \begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/individualprediction.eps}
% 		\caption{Individual prediction}
%    		\label{fig:individualprediction}
% 	\end{subfigure}
%	 ~
%	\begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/naivefusion.eps}
% 		\caption{Naive fusion}
%    		\label{fig:naivefusion}
% 	\end{subfigure}
%	 \\
%	\begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/simplelinearsvm.eps} 			
%		\caption{Fusion using Simple linear SVM}
%    		\label{fig:simplelinearsvm}
% 	\end{subfigure}
%	~
%	\begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/simplerbfsvm.eps} 			
%		\caption{Fusion using Simple RBF SVM}
%    		\label{fig:simplerbfsvm}
% 	\end{subfigure}
%	\\
%	\begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/stackedlinearsvm.eps} 			
%		\caption{Fusion using Stacked Linear SVM}
%    		\label{fig:stackedlinearsvm}
% 	\end{subfigure}
%	~
%	\begin{subfigure}[b]{0.47\textwidth}
% 		\includegraphics[width=1\textwidth]{results/stackedrbfsvm.eps} 			
%		\caption{Fusion using Stacked RBF SVM}
%    		\label{fig:stackedrbfsvm}
% 	\end{subfigure}
%	\caption{Overlap results for the different individual and fusion prediction approaches.}
%	\label{fig:overlapgraph}
%\end{figure}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.48\textwidth]{results1.eps}
		\includegraphics[width=0.48\textwidth]{results2.eps}
		\includegraphics[width=0.48\textwidth]{results3.eps}
		\includegraphics[width=0.48\textwidth]{results4.eps}
		\includegraphics[width=0.48\textwidth]{results5.eps}
		\includegraphics[width=0.48\textwidth]{results6.eps}
		\includegraphics[width=0.48\textwidth]{results7.eps}
		\includegraphics[width=0.48\textwidth]{results8.eps}
	\caption{Examples of results using RBG images comparing the foreground masks obtained with background subtraction (middle) to the final predicted masks (right) with RBF SVM}
	\label{fig:results}
\end{figure}

%overlap discussion:
The obtained results show that, effectively, fusing different descriptions enhances the representation of the scene, thus increasing the final overlap when segmenting subjects and discriminating from other artifacts present in the scene. However, the selection of the fusion approach is crucial. Our proposed naive approach for fusing individual confidences of each modality decreases the overlap of the motion modality - the one that gives the best result among individual predictions -  up to 3\%, but subtantially improves the results if we compare them to the other modalities. On the other hand, as observed by the learning-based fusion experiments, we obtain significant performance improvements regarding each individual modality and the heuristic fusion strategy. As in the subject detection experiment, we achieve the best results using SVM with RBF kernel, which outperforms the motion modality by 6\%.

It is important to note that thermal descriptions cannot reach as good overlap values as the other modalities owing to the fact that the binary masks $FG^{thermal}$ were created from $FG^{depth}$ using the registration algorithm, which cannot be accurate up to pixel level, in such a way that the ground-truth and registered masks will moderately differ, especially in left and right sides of the image. Therefore, we cannot state whether the proposed thermal description performs accurately.

Furthermore, an upward trend is observed as the \emph{don't care region} (DCR) grows, although at higher DCR levels it stabilizes. This is comprehensible because usually the contours of the predicted masks are not accurately defined. Indeed, an accurate pixel-level segmentation is a rather complex task in state of the art techniques.

Having analyzed the experimental results, it is worth investigating the causes of some
misclassifications. One of the problems is originated in the beginning of the chain. Since
background subtraction reduces the search space, it may reject some actual person parts.
That mainly happens when a person is in the same depth than something which belongs
to the background model. Another issue is that some regions considered as unknown,
mostly those generated when one person overlaps other, considerably differ from those
that are used to train the different models. Consequently, the classification of such regions
is not a trivial task.

\section{Conclusions and future work}
\label{sec:conclusions}
A solution for human body segmentation using a calibrated and registered multi-modal data has been proposed. Furthermore, a novel registered and annotated multi-modal RGB-Depth-Thermal dataset of video sequences has been introduced, which contains several
subjects interacting with everyday objects.

We proposed an algorithm to register the different data modalities using multiple homographies generated from several views of the proposed calibration device. In addition, we presented a baseline for segmenting people which starts by an adaptive multi-modal background subtraction approach in order to extract the regions of interest that belong to a user or a moving object in the scene with high confidence. From the set of regions of interest coming
from the different data modalities, different state of the art descriptors have been used
and adapted to describe different feature vectors from each region. In particular, HOG and HOOF have been computed from RGB still images and image sequences, histogram of intensity gradients from thermal data, and histograms of normal vectors from depth maps coming from infrared sensors. The set of descriptors have been selected as the most discriminative ones given the results previously reported in literature.

Despite the obtained results, this proposal leaves further room of improvement. To
begin with, the first background subtraction stage could combine the different modalities
in order to learn the model. Furthermore, a clustering of poses at cell-level could be
added before learning the GMMs. GrabCuts or other post-segmentation approaches could also be applied to the predicted
segmentation binary masks to refine and smooth the contours, which would also produce
a rise in the segmentation accuracy.
{\small
\bibliographystyle{ieee}
\bibliography{references}
}

\end{document}
